{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "729b9678",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_file = ['IS1003c.abssumm.xml',\n",
    " 'IB4003.abssumm.xml',\n",
    " 'ES2008c.abssumm.xml',\n",
    " 'TS3004b.abssumm.xml',\n",
    " 'TS3003b.abssumm.xml',\n",
    " 'IS1005c.abssumm.xml',\n",
    " 'TS3005b.abssumm.xml',\n",
    " 'IS1000d.abssumm.xml',\n",
    " 'TS3010a.abssumm.xml',\n",
    " 'IB4005.abssumm.xml',\n",
    " 'TS3003d.abssumm.xml',\n",
    " 'ES2014d.abssumm.xml',\n",
    " 'TS3004d.abssumm.xml',\n",
    " 'IB4010.abssumm.xml',\n",
    " 'TS3011d.abssumm.xml',\n",
    " 'ES2006d.abssumm.xml',\n",
    " 'ES2002c.abssumm.xml',\n",
    " 'TS3012c.abssumm.xml',\n",
    " 'TS3009b.abssumm.xml',\n",
    " 'ES2007d.abssumm.xml',\n",
    " 'IS1009c.abssumm.xml',\n",
    " 'ES2015d.abssumm.xml',\n",
    " 'ES2011a.abssumm.xml',\n",
    " 'TS3005c.abssumm.xml',\n",
    " 'ES2004a.abssumm.xml',\n",
    " 'TS3003c.abssumm.xml',\n",
    " 'ES2005a.abssumm.xml',\n",
    " 'ES2014c.abssumm.xml',\n",
    " 'TS3011c.abssumm.xml',\n",
    " 'ES2010a.abssumm.xml',\n",
    " 'TS3006d.abssumm.xml',\n",
    " 'IS1004b.abssumm.xml',\n",
    " 'ES2010b.abssumm.xml',\n",
    " 'TS3012b.abssumm.xml',\n",
    " 'IB4011.abssumm.xml',\n",
    " 'IS1006c.abssumm.xml',\n",
    " 'IS1001c.abssumm.xml',\n",
    " 'ES2003b.abssumm.xml',\n",
    " 'IS1004d.abssumm.xml']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0b86b2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-adaf581cd53a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mgood_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwrong_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "from nltk.tokenize import sent_tokenize\n",
    "dir = '/Users/heatherzheng/Desktop/ami_public_manual_1.6.2/abstractive/'\n",
    "\n",
    "good_file = []\n",
    "for file in os.listdir(dir):\n",
    "    if file in wrong_file:\n",
    "        continue\n",
    "    else:\n",
    "        good_file.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "842c62c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "l =len(good_file)\n",
    "result = {}\n",
    "for i in range(5):\n",
    "    \n",
    "    train = []\n",
    "    if i!=4:\n",
    "        test = good_file[int(i*l/5):int((i+1)*l/5)]\n",
    "    else:\n",
    "        test = good_file[int((i)*l/5):]\n",
    "        \n",
    "    if i!=0:\n",
    "        train.extend(good_file[:int((i)*l/5)])\n",
    "        \n",
    "    if i!=4:\n",
    "        train.extend(good_file[int((i+1)*l/5):])\n",
    "        \n",
    "    #check \n",
    "    for f in train:\n",
    "        if f in test:\n",
    "            print(error)\n",
    "        \n",
    "    new_train = train[:int(0.85*len(train))]\n",
    "    new_val = train[int(0.85*len(train)):]\n",
    "    \n",
    "\n",
    "    assert len(new_train)+len(new_val)+len(test)==l\n",
    "    \n",
    "    result[i] = {'train':new_train,'val':new_val,'test':test}\n",
    "    \n",
    "with open('data-resplit/split.json', 'w') as f:\n",
    "    json.dump(result, f) \n",
    "'''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "985ecfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import nltk\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "dir = '/Users/heatherzheng/Desktop/ami_public_manual_1.6.2/extractive/'\n",
    "\n",
    "link_map = defaultdict(list)\n",
    "for file in os.listdir(dir):\n",
    "    if 'summlink' in file and \".#\" not in file:\n",
    "        tree = ET.parse(dir+file)\n",
    "        root = tree.getroot()\n",
    "        for content in root.findall('summlink'):\n",
    "            val = None\n",
    "            for child in content:\n",
    "                if val == None:\n",
    "                    val = child.attrib['href'].split(\"(\")[1].replace(\")\",\"\")\n",
    "                else:\n",
    "                    key = child.attrib['href'].split(\"(\")[1].replace(\")\",\"\")\n",
    "            link_map[key].append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77169d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_da(left,right):\n",
    "    tmp_dir =  '/Users/heatherzheng/Desktop/ami_public_manual_1.6.2/words/'\n",
    "    file = \".\".join(left.split(\".\")[0:2])+\".words\"\n",
    "    tree = ET.parse(tmp_dir+file+\".xml\")\n",
    "    root = tree.getroot()\n",
    "    left = int(left.replace(file,\"\"))\n",
    "    time = float('inf')\n",
    "    \n",
    "    if right ==None:\n",
    "        right = left\n",
    "    else:\n",
    "        right = int(right.replace(file,\"\"))\n",
    "    da = []\n",
    "    for content in root.findall('w'):\n",
    "        id = int(content.attrib['{http://nite.sourceforge.net/}id'].replace(file,\"\"))\n",
    "        if id>= left and id <=right:\n",
    "            time = min(time,float(content.get('starttime')))\n",
    "            value = content.text\n",
    "            #key = content.get('starttime')\n",
    "            da.append(value.replace(\"_\",\"\"))\n",
    "    return \" \".join(da),time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47e0db19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_drda(file_name, da):\n",
    "    tmp_dir =  '/Users/heatherzheng/Desktop/ami_public_manual_1.6.2/dialogueActs/'\n",
    "    tree = ET.parse(tmp_dir+\".\".join(da.split(\".\")[0:3])+\".xml\")\n",
    "    root = tree.getroot()\n",
    "    for content in root.findall('dact'):\n",
    "        id = content.attrib['{http://nite.sourceforge.net/}id']\n",
    "        if id == da:\n",
    "            drda_type = None\n",
    "            for node in content:\n",
    "                if 'role' in node.attrib:\n",
    "                    drda_type = node.attrib['href'].split(\"_\")[-1].split(\")\")[0]\n",
    "                if 'role' not in node.attrib:\n",
    "                    href = node.attrib['href']\n",
    "                    left,right =None,None\n",
    "                    for val in href.split(\"#\")[1].split(\"..\"):\n",
    "                        if left==None:\n",
    "                            left = val.replace(\"id(\",\"\").replace(\")\",\"\")\n",
    "                        else:\n",
    "                            right = val.replace(\"id(\",\"\").replace(\")\",\"\")\n",
    "            \n",
    "            return left,right, drda_type\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e28c1ba0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_transcript' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3090c4ef5181>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m \u001b[0mgenerate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-3090c4ef5181>\u001b[0m in \u001b[0;36mgenerate_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtfidf_input\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_transcript\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mspeaker\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                     \u001b[0mtfidf_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pair_ntm_sentence_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_split\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"tfidf.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_transcript' is not defined"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "\n",
    "all_summaries = []\n",
    "def find_context(sent, corpus_tfidf):\n",
    "    max_val = 0\n",
    "    max_pos = None\n",
    "    for i in range(len(corpus_tfidf)):\n",
    "        val = cosine_similarity(sent,[corpus_tfidf[i]])[0][0]\n",
    "        if val > max_val:\n",
    "            max_val = val\n",
    "            max_pos = i\n",
    "            \n",
    "    second_val = -0.1\n",
    "    second_pos = None\n",
    "    \n",
    "    for i in range(len(corpus_tfidf)):\n",
    "        val = cosine_similarity(sent,[corpus_tfidf[i]])[0][0]\n",
    "        if i==max_pos:\n",
    "            continue\n",
    "        if val > second_val:\n",
    "            second_val = val\n",
    "            second_pos = i\n",
    "        \n",
    "    if second_pos ==None:\n",
    "        print(second_val)\n",
    "        print(len(corpus_tfidf))\n",
    "    return second_pos\n",
    "\n",
    "def generate_data():\n",
    "    avg = []\n",
    "    data = []\n",
    "    all_sum = []\n",
    "    f_name = []\n",
    "    \n",
    "    for cross_split in range(5):\n",
    "        tfidf_input = []\n",
    "        tfidf = None\n",
    "        for mode in [\"train\",\"val\",\"test\"]:\n",
    "            try:\n",
    "                os.mkdir('pair_ntm_sentence_'+str(cross_split+1))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "            try:\n",
    "                os.mkdir('pair_ntm_sentence_'+str(cross_split+1)+\"/\"+mode)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "                \n",
    "            pos_example, neg_example = 0,0\n",
    "            r = []\n",
    "            for file in os.listdir(\"data_nice_split/\"+'pair_ntm_sentence_'+str(cross_split+1)+\"/\"+mode):\n",
    "                r.append(file.split(\".\")[0]+'.abssumm.xml')\n",
    "\n",
    "            if tfidf_input == []:\n",
    "                for file in r:\n",
    "                    _,_, corpus,_ = generate_transcript(files[file.split('.')[0]],speaker[file.split('.')[0]])\n",
    "                    tfidf_input.extend(corpus)\n",
    "                with open('pair_ntm_sentence_'+str(cross_split+1)+\"/\"+\"tfidf.json\", 'w') as f:\n",
    "                    json.dump(tfidf_input, f)               \n",
    "                vectorizer = TfidfVectorizer()\n",
    "                tfidf = vectorizer.fit(tfidf_input)\n",
    "                    \n",
    "            for file in r:\n",
    "                dir = '/Users/heatherzheng/Desktop/ami_public_manual_1.6.2/abstractive/'\n",
    "                tree = ET.parse(dir+file)\n",
    "                root = tree.getroot()\n",
    "\n",
    "                \n",
    "                transcript, key, corpus,_ = generate_transcript(files[file.split('.')[0]],speaker[file.split('.')[0]])\n",
    "                \n",
    "                corpus_tfidf = tfidf.transform(corpus)\n",
    "                text = []\n",
    "                sent_dict = defaultdict(str)\n",
    "                flag = True\n",
    "                group = 0\n",
    "                \n",
    "                drda = []\n",
    "                stats = []\n",
    "                \n",
    "                for content in root.findall('decisions'):\n",
    "                    for sentence in content.findall('sentence'):\n",
    "                        summary = sentence.text.replace(\"\\n\",\"\")\n",
    "                        all_summaries.append(summary)\n",
    "                        num_sent = 0\n",
    "                        group += 1\n",
    "                        f_name.append(file)\n",
    "                        decision_id = sentence.attrib['{http://nite.sourceforge.net/}id']\n",
    "                        ext_link = link_map[decision_id]\n",
    "                        if ext_link ==[]:\n",
    "                            flag = False\n",
    "                        else:\n",
    "                            for i in ext_link:\n",
    "                                left,right,drda_type = get_drda(file, i)\n",
    "                                da,time = get_da(left,right)\n",
    "                                das = sent_detector.tokenize(da)\n",
    "                                for comp in das:\n",
    "                                    if search_trans(transcript, key, comp)==None:\n",
    "                                        #print(comp)\n",
    "                                        #print(transcript)\n",
    "                                        comp = comp.replace(\". \",\"\")\n",
    "                                    if 'just kinetic then' in comp:\n",
    "                                        comp = 'just kinetic then'\n",
    "\n",
    "                                    pos, sentence, s_time = search_trans(transcript, key, comp)\n",
    "                                    #print(s_time,time)\n",
    "                                    if sentence ==\"\":\n",
    "                                        continue\n",
    "                                    if pos not in sent_dict:\n",
    "                                        if len(sentence.split())<3:\n",
    "                                            continue\n",
    "                                        context_s = corpus[find_context(tfidf.transform([sentence]).toarray(),corpus_tfidf.toarray())]\n",
    "                                        text.append((s_time,sentence,group,pos,context_s,summary))\n",
    "                                        stats.append(group)\n",
    "                                        drda.append(sentence)\n",
    "                                        num_sent +=1\n",
    "                                    sent_dict[pos] = sentence\n",
    "                        avg.append(num_sent)\n",
    "                text.sort(key = lambda x: x[0]) \n",
    "                \n",
    "                #pos/neg calculation\n",
    "                c = list(dict(Counter(stats)).values())\n",
    "                for i in range(len(c)):\n",
    "                    if c[i]>1:\n",
    "                        pos_example += c[i]*(c[i]-1)/2.0\n",
    "                    for j in range(i+1,len(c)):\n",
    "                        neg_example += c[i]*c[j]\n",
    "                                       \n",
    "                result = []\n",
    "                for i in range(len(text)):\n",
    "                    #sentence_pos,time, sentence, group, pos over passage, context sentence, drdas,summaries\n",
    "                    result.append((i,text[i][0],text[i][1],text[i][2],text[i][3], text[i][4],\" \".join(drda),text[i][5]))\n",
    "                for i in text:\n",
    "                    with open('pair_ntm_sentence_'+str(cross_split+1)+\"/\"+mode+\"/\"+file.split(\".\")[0]+\".json\", 'w') as f:\n",
    "                        json.dump(result, f)\n",
    "\n",
    "    print(sum(avg)/len(avg))\n",
    "generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "860161d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_transcript' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3186bde73273>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    113\u001b[0m                     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"transcript\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"summary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_summaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m \u001b[0mgenerate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-3186bde73273>\u001b[0m in \u001b[0;36mgenerate_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtfidf_input\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_transcript\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mspeaker\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m                     \u001b[0mtfidf_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pair_ntm_sentence_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_split\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"tfidf.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_transcript' is not defined"
     ]
    }
   ],
   "source": [
    "##final all together data\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "\n",
    "all_summaries = []\n",
    "def find_context(sent, corpus_tfidf):\n",
    "    max_val = 0\n",
    "    max_pos = None\n",
    "    for i in range(len(corpus_tfidf)):\n",
    "        val = cosine_similarity(sent,[corpus_tfidf[i]])[0][0]\n",
    "        if val > max_val:\n",
    "            max_val = val\n",
    "            max_pos = i\n",
    "            \n",
    "    second_val = -0.1\n",
    "    second_pos = None\n",
    "    \n",
    "    for i in range(len(corpus_tfidf)):\n",
    "        val = cosine_similarity(sent,[corpus_tfidf[i]])[0][0]\n",
    "        if i==max_pos:\n",
    "            continue\n",
    "        if val > second_val:\n",
    "            second_val = val\n",
    "            second_pos = i\n",
    "        \n",
    "    if second_pos ==None:\n",
    "        print(second_val)\n",
    "        print(len(corpus_tfidf))\n",
    "    return second_pos\n",
    "\n",
    "def generate_data():\n",
    "    avg = []\n",
    "    data = []\n",
    "    f_name = []\n",
    "    \n",
    "    for cross_split in range(1):\n",
    "        tfidf_input = []\n",
    "        tfidf = None\n",
    "        for mode in [\"train\",\"val\",\"test\"]:\n",
    "            try:\n",
    "                os.mkdir('pair_ntm_sentence_'+str(cross_split+1))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "            try:\n",
    "                os.mkdir('pair_ntm_sentence_'+str(cross_split+1)+\"/\"+mode)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            r = []\n",
    "            for file in os.listdir(\"data_nice_split/\"+'pair_ntm_sentence_'+str(cross_split+1)+\"/\"+mode):\n",
    "                r.append(file.split(\".\")[0]+'.abssumm.xml')\n",
    "\n",
    "            if tfidf_input == []:\n",
    "                for file in r:\n",
    "                    _,_, corpus,_ = generate_transcript(files[file.split('.')[0]],speaker[file.split('.')[0]])\n",
    "                    tfidf_input.extend(corpus)\n",
    "                with open('pair_ntm_sentence_'+str(cross_split+1)+\"/\"+\"tfidf.json\", 'w') as f:\n",
    "                    json.dump(tfidf_input, f)               \n",
    "                vectorizer = TfidfVectorizer()\n",
    "                tfidf = vectorizer.fit(tfidf_input)\n",
    "                    \n",
    "            for file in r:\n",
    "                dir = '/Users/heatherzheng/Desktop/ami_public_manual_1.6.2/abstractive/'\n",
    "                tree = ET.parse(dir+file)\n",
    "                root = tree.getroot()\n",
    "\n",
    "                \n",
    "                transcript, key, corpus,_ = generate_transcript(files[file.split('.')[0]],speaker[file.split('.')[0]])\n",
    "                \n",
    "                corpus_tfidf = tfidf.transform(corpus)\n",
    "                text = []\n",
    "                sent_dict = defaultdict(str)\n",
    "                flag = True\n",
    "                group = 0\n",
    "                \n",
    "                drda = []\n",
    "                stats = []\n",
    "                all_summaries = []\n",
    "                \n",
    "                \n",
    "                for content in root.findall('decisions'):\n",
    "                    for sentence in content.findall('sentence'):\n",
    "                        summary = sentence.text.replace(\"\\n\",\"\")\n",
    "                        all_summaries.append(summary)\n",
    "                        num_sent = 0\n",
    "                        \n",
    "                pos = 0\n",
    "                for i in key:\n",
    "                    sentence = []\n",
    "                    for word in transcript[i][2:].split():\n",
    "                        if word.lower() in utterance:\n",
    "                            continue\n",
    "                        else:\n",
    "                            sentence.append(word)\n",
    "                    if len(sentence) < 3:\n",
    "                        continue\n",
    "                    else:\n",
    "                        sentence = \" \".join(sentence)\n",
    "                    s_time = i\n",
    "                    context_s = corpus[find_context(tfidf.transform([sentence]).toarray(),corpus_tfidf.toarray())]\n",
    "                    text.append((s_time,sentence,1,pos,context_s,\"\"))\n",
    "                    pos += 1\n",
    "                         \n",
    "                result = []\n",
    "                for i in range(len(text)):\n",
    "                    #da_pos,time, sentence, group, pos over passage, context sentence, drdas,summaries\n",
    "                    result.append((i,text[i][0],text[i][1],text[i][2],text[i][3], text[i][4],\"\",text[i][5]))\n",
    "\n",
    "                with open('pair_ntm_sentence_'+str(cross_split+1)+\"/\"+mode+\"/\"+file.split(\".\")[0]+\".json\", 'w') as f:\n",
    "                    json.dump({\"transcript\":result,\"summary\":\" \".join(all_summaries)}, f)\n",
    "\n",
    "generate_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "880efc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import collections\n",
    "import xml.etree.ElementTree as ET\n",
    "import string\n",
    "import html\n",
    "import nltk.data\n",
    "\n",
    "\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "utterance = ['uh','hmm','um','mm','huh','mm-hmm']\n",
    "\n",
    "def generate_transcript(file_list,speakers):\n",
    "    mydict = {}\n",
    "    corpus = []\n",
    "    endtime = {}\n",
    "    dir = '/Users/heatherzheng/Desktop/ami_public_manual_1.6.2/words/'\n",
    "    for n in range(len(file_list)):\n",
    "        tree = ET.parse(dir+file_list[n])\n",
    "        root = tree.getroot()\n",
    "        token = []\n",
    "        time = []\n",
    "\n",
    "        for content in root.findall('w'):\n",
    "            value = content.text\n",
    "            key = content.get('starttime')\n",
    "            #if value.lower() in utterance:\n",
    "                #continue\n",
    "            if len(token)>=1 and token[-1] in string.punctuation and value in string.punctuation:\n",
    "                continue\n",
    "            #if value.lower() in utterance:\n",
    "               # continue\n",
    "            if key == None:\n",
    "                continue\n",
    "            token.append(value)\n",
    "            time.append(key)\n",
    "\n",
    "        sentences = sent_detector.tokenize(\" \".join(token))\n",
    "        count = 0\n",
    "        for sentence in sentences:\n",
    "            mydict[float(time[count])] = speakers[n] +\":\"+sentence.replace(\"_\",\"\")\n",
    "            corpus.append(sentence.replace(\"_\",\"\"))\n",
    "            prev_count = count\n",
    "            count += len(sentence.split(\" \"))\n",
    "            endtime[float(time[prev_count])] = time[count-1]\n",
    "\n",
    "\n",
    "\n",
    "    mydict_sort = sorted(mydict)\n",
    "    remove_lst = []\n",
    "    for key in mydict_sort:\n",
    "        sentence = mydict[key]\n",
    "        sentence = \" \".join([word.lower() for word in sentence[2:].split() if word.lower() not in utterance])\n",
    "        if not any(c.isalpha() for c in sentence):\n",
    "            remove_lst.append(key)\n",
    "            del mydict[key]\n",
    "    \n",
    "    for i in remove_lst:\n",
    "        mydict_sort.remove(i)\n",
    "        \n",
    "    new_corpus = []\n",
    "    for key in mydict_sort:\n",
    "        new_corpus.append(mydict[key])\n",
    "        \n",
    "\n",
    "    return mydict,mydict_sort,new_corpus,endtime\n",
    "\n",
    "    '''\n",
    "    insert_lines = []\n",
    "\n",
    "    for i in mydict_sort:\n",
    "        sentence = mydict[i].split(':')[0]\n",
    "    '''\n",
    "        \n",
    "\n",
    "\n",
    "files = collections.defaultdict(list)\n",
    "speaker  =collections.defaultdict(list)\n",
    "dir = '/Users/heatherzheng/Desktop/ami_public_manual_1.6.2/words/'\n",
    "for f in os.listdir(dir):\n",
    "    files[f.split('.')[0]].append(f)\n",
    "    speaker[f.split('.')[0]].append(f.split('.')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ddac535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_trans(transcript, key, da,time):\n",
    "    #print(da)\n",
    "    #print(\"___\")\n",
    "    \n",
    "    #print(transcript)\n",
    "    #print(da)\n",
    "    count = 0 \n",
    "    lookup = []\n",
    "    for item,i in enumerate(key):\n",
    "        sentence = transcript[i][2:]\n",
    "        \n",
    "        if da.lower() in sentence.lower():\n",
    "            #print(sentence)\n",
    "            diff = round(abs(i-time),2)\n",
    "            lookup.append((diff,i,item))\n",
    " \n",
    "    lookup.sort(key=lambda x:x[0])\n",
    "    \n",
    "    clean_s = []\n",
    "    if lookup==[]:\n",
    "        print(transcript,da)\n",
    "        \n",
    "    for word in transcript[lookup[0][1]][2:].split():\n",
    "        if word.lower() in utterance:\n",
    "            continue\n",
    "        else:\n",
    "            clean_s.append(word.lower())\n",
    "    if len(clean_s)<2:\n",
    "        return count,\"\",\"\"\n",
    "    return lookup[0][2],transcript[lookup[0][1]][:2]+\" \"+\" \".join(clean_s),lookup[0][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3acedd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "sentence_summary_map = {}\n",
    "def ntm_data(split_num=0,dtype='test'):\n",
    "    data = []\n",
    "    all_sum = []\n",
    "    f_name = []\n",
    "\n",
    "    true_label = []\n",
    "    train_sentence = []\n",
    "    r = []\n",
    "\n",
    "    f = open ('data-resplit/split.json', \"r\")\n",
    "  \n",
    "\n",
    "    data = json.loads(f.read())[str(split_num)]\n",
    "    r = data[dtype]\n",
    "\n",
    "\n",
    "    alldata = []\n",
    "    posn=0\n",
    "    negn = 0\n",
    "    for file in r:        \n",
    "        dir = '/Users/heatherzheng/Desktop/ami_public_manual_1.6.2/abstractive/'\n",
    "        tree = ET.parse(dir+file)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        transcript, key,corpus,endtime = generate_transcript(files[file.split('.')[0]],speaker[file.split('.')[0]])\n",
    "\n",
    "        sent_dict = defaultdict(str)\n",
    "        flag = True\n",
    "        \n",
    "        selected = []\n",
    "        summaries = []\n",
    "        alldas = []\n",
    "        allsavedkey = []\n",
    "        \n",
    "        tmp_c = 0\n",
    "        for content in root.findall('decisions'):\n",
    "            for sentence in content.findall('sentence'):\n",
    "                summary = sentence.text.replace(\"\\n\",\"\")\n",
    "                all_sum.append(summary)\n",
    "                f_name.append(file)\n",
    "                decision_id = sentence.attrib['{http://nite.sourceforge.net/}id']\n",
    "                ext_link = link_map[decision_id]\n",
    "\n",
    "                if ext_link ==[]:\n",
    "                    flag = False\n",
    "                else:\n",
    "                    for i in ext_link:\n",
    "                        left,right,_ = get_drda(file, i)\n",
    "                        da,time = get_da(left,right)\n",
    "                        alldas.append(da)\n",
    "                        das = sent_detector.tokenize(da)\n",
    "\n",
    "                        for comp in das:\n",
    "                            if len(comp.split())<3:\n",
    "                                continue\n",
    "                            if 'just kinetic then' in comp:\n",
    "                                comp = 'just kinetic then'\n",
    "                            pos, sentence, savedkey = search_trans(transcript, key, comp,time)\n",
    "                            if sentence ==\"\":\n",
    "                                print(comp)\n",
    "                                continue\n",
    "                            if sentence not in selected:\n",
    "                                tmp_c +=1\n",
    "                                selected.append(sentence)\n",
    "                                allsavedkey.append(savedkey)\n",
    "                                #sentence_summary_map[sentence] = summary\n",
    "                                #tmp.append(sentence)\n",
    "        kk = []\n",
    "        file_train_sentence = []\n",
    "        file_true_label = []\n",
    "        posn = 0\n",
    "        for i in key:\n",
    "            sentence = transcript[i]\n",
    "            ori = sentence\n",
    "            sentence = \" \".join([word.lower() for word in sentence[2:].split() if word.lower() not in utterance])\n",
    "            #print(sentence)\n",
    "            if any(c.isalpha() for c in sentence):\n",
    "                file_train_sentence.append(ori[0:2]+\" \"+sentence)\n",
    "                if i in allsavedkey:\n",
    "                    file_true_label.append(True)\n",
    "                    posn +=1\n",
    "                else:\n",
    "                    file_true_label.append(False)\n",
    "                    negn +=1\n",
    "\n",
    "        true_label.append(file_true_label)\n",
    "        train_sentence.append(file_train_sentence)\n",
    "        assert len(file_true_label) == len(file_train_sentence)\n",
    "    \n",
    "    assert len(true_label) == len(train_sentence)\n",
    "    with open('sentence_extraction_train/'+str(split_num)+\"_seq_model_\"+dtype+'.json', 'w') as f:\n",
    "        json.dump([train_sentence,true_label], f)     \n",
    "    #with open('sentence_extraction_train/'+str(split_num)+\"_\"+dtype+\"_summary\"+'.json', 'w') as f:\n",
    "        #json.dump(all_sum, f)        \n",
    "\n",
    "    ''''\n",
    "    for i in text:\n",
    "        file1 = open(\"meetingsentence_lines_test.txt\", \"a\")  # append mode\n",
    "        file1.write(i+\"\\n\")\n",
    "        file1.close()\n",
    "    '''\n",
    "    #f = open(\"sentence_to_summary.json\",\"w\")\n",
    "    #json.dump(sentence_summary_map, f)\n",
    "    \n",
    "for split_num in [2,3,4]:\n",
    "    for dtype in ['train','val','test']:\n",
    "        ntm_data(split_num=split_num, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "fbd02092",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open ('test_15_sent.json', \"r\")\n",
    "\n",
    "\n",
    "train = json.loads(f.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "166c3f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_f1_score(y_true, y_pred):\n",
    "    a = 0\n",
    "    for i in y_pred:\n",
    "        if i in y_true:\n",
    "            a +=1\n",
    "            \n",
    "    p = a/len(y_true)\n",
    "    r = a/len(y_pred)\n",
    "    try:\n",
    "        val = 2*p*r/(p+r)\n",
    "    except:\n",
    "        val = 0\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a6e9f5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21428571428571427\n",
      "0.13655462184873948\n",
      "0.12133944486885662\n",
      "0.09100458365164246\n",
      "0.09385629850026134\n",
      "0.15397115784112686\n",
      "0.16558872352768855\n",
      "0.16572346642006083\n",
      "0.19175419237338742\n",
      "0.18507877313604867\n",
      "0.18643524830549882\n",
      "0.17847473518913148\n",
      "0.20577155043099316\n",
      "0.2139307254002079\n",
      "0.19966867704019406\n",
      "0.19343938472518193\n",
      "0.18859654510082483\n",
      "0.17811895926189011\n",
      "0.17576182105512397\n",
      "0.18602134904998682\n",
      "0.1883676713641331\n",
      "1651.0 554\n",
      "0.1883676713641331\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "\n",
    "\n",
    "files = collections.defaultdict(list)\n",
    "speaker  =collections.defaultdict(list)\n",
    "dir = '/Users/heatherzheng/Desktop/ami_public_manual_1.6.2/words/'\n",
    "for f in os.listdir(dir):\n",
    "    files[f.split('.')[0]].append(f)\n",
    "    speaker[f.split('.')[0]].append(f.split('.')[1])\n",
    "    \n",
    "\n",
    "def find_context(sent, corpus_tfidf):\n",
    "    max_val = 0\n",
    "    max_pos = None\n",
    "    for i in range(len(corpus_tfidf)):\n",
    "        val = cosine_similarity(sent,[corpus_tfidf[i]])[0][0]\n",
    "        if val > max_val:\n",
    "            max_val = val\n",
    "            max_pos = i\n",
    "            \n",
    "    second_val = -0.1\n",
    "    second_pos = None\n",
    "    \n",
    "    for i in range(len(corpus_tfidf)):\n",
    "        val = cosine_similarity(sent,[corpus_tfidf[i]])[0][0]\n",
    "        if i==max_pos:\n",
    "            continue\n",
    "        if val > second_val:\n",
    "            second_val = val\n",
    "            second_pos = i\n",
    "        \n",
    "    if second_pos ==None:\n",
    "        print(second_val)\n",
    "        print(len(corpus_tfidf))\n",
    "    return second_pos\n",
    "\n",
    "def generate_data(split_num=1,dtype='test'):\n",
    "    avg = []\n",
    "    data = []\n",
    "    all_sum = []\n",
    "    f_name = []\n",
    "    count = 0 \n",
    " \n",
    "    with open(os.path.join(\"sentence_extraction_train\",'1_seq_model_test.json'), 'r') as f:\n",
    "        seq = json.loads(f.read())\n",
    "        \n",
    "    f = open ('data-resplit/split.json', \"r\")  \n",
    "\n",
    "    pos_example,neg_example =0,0\n",
    "    data = json.loads(f.read())[str(split_num)]\n",
    "    r = data[dtype]\n",
    "\n",
    "    tfidf_input = []\n",
    "\n",
    "    for file in data['train']:\n",
    "\n",
    "        _,_, corpus,_ = generate_transcript(files[file.split(\".\")[0]],speaker[file.split(\".\")[0]])\n",
    "        \n",
    "        tfidf_input.extend(corpus)\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf = vectorizer.fit(tfidf_input)\n",
    "    f1score = []\n",
    "    for file in r:\n",
    "        select_sentence = train[count]\n",
    "        count +=1\n",
    "        dir = '/Users/heatherzheng/Desktop/ami_public_manual_1.6.2/abstractive/'\n",
    "        tree = ET.parse(dir+file)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        \n",
    "        transcript, key, corpus,_ = generate_transcript(files[file.split(\".\")[0]],speaker[file.split(\".\")[0]])\n",
    "        \n",
    "        \n",
    "        corpus_tfidf = tfidf.transform(corpus)\n",
    "        text = []\n",
    "        sent_dict = defaultdict(str)\n",
    "        flag = True\n",
    "        group = 0\n",
    "\n",
    "        drda = []\n",
    "        stats = []\n",
    "\n",
    "        all_summaries,inputs = [],[]\n",
    "\n",
    "    \n",
    "        for content in root.findall('decisions'):\n",
    "            for sentence in content.findall('sentence'):\n",
    "                tmp = []\n",
    "                summary = sentence.text.replace(\"\\n\",\"\")\n",
    "                all_summaries.append(summary)\n",
    "                f_name.append(file)\n",
    "                decision_id = sentence.attrib['{http://nite.sourceforge.net/}id']\n",
    "                ext_link = link_map[decision_id]\n",
    "                if ext_link ==[]:\n",
    "                    flag = False\n",
    "                else:\n",
    "                    for i in ext_link:\n",
    "                        left,right,drda_type = get_drda(file, i)\n",
    "                        da,time = get_da(left,right)\n",
    "                        das = sent_detector.tokenize(da)\n",
    "                        for comp in das:\n",
    "                            if len(comp.split())<3:\n",
    "                                continue\n",
    "                            if 'just kinetic then' in comp:\n",
    "                                comp = 'just kinetic then'\n",
    "                            if search_trans(transcript, key, comp,time)==None:\n",
    "                                #print(comp)\n",
    "                                #print(transcript)\n",
    "                                comp = comp.replace(\". \",\"\")\n",
    "\n",
    "\n",
    "                            pos, sentence, s_time = search_trans(transcript, key, comp,time)\n",
    "                            tmp.append(s_time)\n",
    "                             \n",
    "                        \n",
    "                     \n",
    "                inputs.append(tmp)               \n",
    "        #assert sum(seq[1][count-1]) ==len(set([item for sublist in inputs for item in sublist]))\n",
    "        \n",
    "        fast_rl_group = []\n",
    "        for _ in range(len(inputs)+1):\n",
    "            fast_rl_group.append([])\n",
    "\n",
    "        ttt = []\n",
    "        for item,k in enumerate(key):    \n",
    "            if item in select_sentence:\n",
    "                group = -1\n",
    "                for gidx,g in enumerate(inputs):\n",
    "                    if k in g:\n",
    "                        group = gidx\n",
    "                fast_rl_group[group].append([transcript[k],k,item])\n",
    "            for gidx,g in enumerate(inputs):\n",
    "                if k in g:\n",
    "                    ttt.append(item)\n",
    "\n",
    "        if inputs==[]:\n",
    "            f1score.append(0)\n",
    "        else:\n",
    "            f1score.append(my_f1_score(list(set(ttt)), select_sentence))\n",
    "\n",
    "        aseertnum2 = 0\n",
    "        \n",
    "        fast_rl_group = [i for i in fast_rl_group if i!=[]]\n",
    "        \n",
    "        for groupidx,group in enumerate(fast_rl_group):\n",
    "            for sentence,time,pos in group:\n",
    "                if len(sentence.split())<3:\n",
    "                    continue\n",
    "                context_s = corpus[find_context(tfidf.transform([sentence]).toarray(),corpus_tfidf.toarray())]\n",
    "                if groupidx>=len(all_summaries):\n",
    "                    tmpsum=\"\"\n",
    "                else:\n",
    "                    tmpsum = all_summaries[groupidx]\n",
    "                text.append((time,sentence,groupidx,pos,context_s,tmpsum))\n",
    "                if groupidx!=len(inputs):\n",
    "                    aseertnum2 +=1\n",
    "                stats.append(groupidx)\n",
    "                drda.append(sentence)\n",
    "\n",
    "        #assert aseertnum2 ==assertnum1\n",
    "        text.sort(key = lambda x: x[0]) \n",
    "\n",
    "        #pos/neg calculation\n",
    "\n",
    "        c = list(dict(Counter(stats)).values())\n",
    "        for i in range(len(c)):\n",
    "            if c[i]>1:\n",
    "                pos_example += c[i]*(c[i]-1)/2.0\n",
    "            for j in range(i+1,len(c)):\n",
    "                neg_example += c[i]*c[j]\n",
    "\n",
    "        result = []\n",
    "        for i in range(len(text)):\n",
    "            #sentence_pos,time, sentence, group, pos over passage, context sentence, drdas,summaries\n",
    "            result.append((i,text[i][0],text[i][1],text[i][2],text[i][3], text[i][4],\" \".join(drda),text[i][5]))\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in text:\n",
    "            with open('new_cluster_data'+\"/\"+dtype+\"_15_sent/\"+file.split(\".\")[0]+\".json\", 'w') as f:\n",
    "                json.dump(result, f)\n",
    "        print(sum(f1score)/len(f1score))\n",
    "    print(pos_example,neg_example)\n",
    "    print(sum(f1score)/len(f1score))\n",
    "\n",
    "generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "952eb582",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_summ = []\n",
    "len_input = []\n",
    "def generate_data(split_num=1,dtype='test'):\n",
    "    avg = []\n",
    "    data = []\n",
    "    all_sum = []\n",
    "    f_name = []\n",
    "    count = 0 \n",
    " \n",
    "    f = open ('data-resplit/split.json', \"r\")  \n",
    "\n",
    "    data = json.loads(f.read())[str(split_num)]\n",
    "    r = data[dtype]+data['val']+data['train']\n",
    "\n",
    "    file_summary = {}\n",
    "    file_summary_order = []\n",
    "    all_text = []\n",
    "    for file in r:\n",
    "\n",
    "        dir = '/Users/heatherzheng/Desktop/ami_public_manual_1.6.2/abstractive/'\n",
    "        tree = ET.parse(dir+file)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        \n",
    "        transcript, key, corpus,_ = generate_transcript(files[file.split(\".\")[0]],speaker[file.split(\".\")[0]])\n",
    "        \n",
    "\n",
    "        \n",
    "        flag = True\n",
    "        group = 0\n",
    "\n",
    "        drda = []\n",
    "        stats = []\n",
    "\n",
    "        tmp_sum = []\n",
    "        \n",
    "        \n",
    "        for content in root.findall('decisions'):\n",
    "            for sentence in content.findall('sentence'):\n",
    "                text = []\n",
    "                sent_dict = defaultdict(str)\n",
    "                \n",
    "                \n",
    "                summary = sentence.text.replace(\"\\n\",\"\")\n",
    "                tmp_sum.append(summary)\n",
    "                file_summary_order.append(summary)\n",
    "                len_summ.append(len(summary.split()))\n",
    "\n",
    "                decision_id = sentence.attrib['{http://nite.sourceforge.net/}id']\n",
    "                #print(decision_id)\n",
    "                ext_link = link_map[decision_id]\n",
    "\n",
    "                if ext_link ==[]:\n",
    "                    flag = False\n",
    "                else:\n",
    "                    for i in ext_link:\n",
    "                        left,right,_ = get_drda(file, i)\n",
    "                        da,time = get_da(left,right)\n",
    "                        das = sent_detector.tokenize(da)\n",
    "                        for comp in das:\n",
    "                            if len(comp.split())<3:\n",
    "                                continue\n",
    "                            if 'just kinetic then' in comp:\n",
    "                                comp = 'just kinetic then'\n",
    "                            if search_trans(transcript, key, comp,time)==None:\n",
    "                                #print(comp)\n",
    "                                #print(transcript)\n",
    "                                comp = comp.replace(\". \",\"\")\n",
    "\n",
    "                            pos, sentence, s_time = search_trans(transcript, key, comp,time)\n",
    "\n",
    "                            if sentence ==\"\":\n",
    "                                print(comp)\n",
    "                                continue\n",
    "\n",
    "                            if pos not in sent_dict:\n",
    "                                text.append(sentence[3:])\n",
    "                            sent_dict[pos] = sentence\n",
    "        \n",
    "                all_text.append(\" \".join(text))  \n",
    "                len_input.append(len(\" \".join(text).split()))\n",
    "\n",
    "        file_summary[file.split(\".\")[0]] = \" \".join(tmp_sum)  \n",
    "\n",
    "        #len_summ.append(len(\" \".join(tmp_sum).split()))\n",
    "        #len_input.append(len(\" \".join(text).split()))\n",
    "\n",
    "    assert len(all_text)==len(file_summary_order)\n",
    "    \n",
    "    assert len(file_summary)==len(r)\n",
    "    \n",
    "    if dtype=='test':\n",
    "        with open('bart_data/test_sum_gold.json', 'w') as f:\n",
    "            json.dump(file_summary, f)     \n",
    "\n",
    "    '''\n",
    "    for i in range(len(file_summary)):\n",
    "        file1 = open(\"bart_data/cluster_\"+str(split_num)+\"_\"+dtype+\".txt.src\",\"a\")\n",
    "        file1.write(all_text[i]+\"\\n\")\n",
    "        file2 = open(\"bart_data/cluster_\"+str(split_num)+\"_\"+dtype+\".txt.tgt.tagged\",\"a\")\n",
    "        file2.write(file_summary_order[i]+\"\\n\")\n",
    "\n",
    "    '''\n",
    "generate_data()                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e7035f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(len_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2bce8b6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(len_summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a80a6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_summ = []\n",
    "len_input = []\n",
    "def generate_data(split_num=1,dtype='test'):\n",
    "    avg = []\n",
    "    data = []\n",
    "    all_sum = []\n",
    "    f_name = []\n",
    "    count = 0 \n",
    " \n",
    "    f = open ('data-resplit/split.json', \"r\")  \n",
    "\n",
    "    data = json.loads(f.read())[str(split_num)]\n",
    "    r = data[dtype]\n",
    "\n",
    "    file_summary = {}\n",
    "    file_summary_order = []\n",
    "    all_text = []\n",
    "    for file in r:\n",
    "\n",
    "        dir = '/Users/heatherzheng/Desktop/ami_public_manual_1.6.2/abstractive/'\n",
    "        tree = ET.parse(dir+file)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        \n",
    "        transcript, key, corpus,_ = generate_transcript(files[file.split(\".\")[0]],speaker[file.split(\".\")[0]])\n",
    "        \n",
    "\n",
    "        \n",
    "        flag = True\n",
    "        group = 0\n",
    "\n",
    "        drda = []\n",
    "        stats = []\n",
    "\n",
    "        tmp_sum = []\n",
    "        \n",
    "        text = []\n",
    "        sent_dict = defaultdict(str)\n",
    "        for content in root.findall('decisions'):\n",
    "            for sentence in content.findall('sentence'):\n",
    "\n",
    "                summary = sentence.text.replace(\"\\n\",\"\")\n",
    "                tmp_sum.append(summary)\n",
    "\n",
    "                decision_id = sentence.attrib['{http://nite.sourceforge.net/}id']\n",
    "                #print(decision_id)\n",
    "                ext_link = link_map[decision_id]\n",
    "\n",
    "                if ext_link ==[]:\n",
    "                    flag = False\n",
    "                else:\n",
    "                    for i in ext_link:\n",
    "                        left,right,_ = get_drda(file, i)\n",
    "                        da,time = get_da(left,right)\n",
    "                        das = sent_detector.tokenize(da)\n",
    "                        for comp in das:\n",
    "                            if len(comp.split())<3:\n",
    "                                continue\n",
    "                            if 'just kinetic then' in comp:\n",
    "                                comp = 'just kinetic then'\n",
    "                            if search_trans(transcript, key, comp,time)==None:\n",
    "                                #print(comp)\n",
    "                                #print(transcript)\n",
    "                                comp = comp.replace(\". \",\"\")\n",
    "\n",
    "                            pos, sentence, s_time = search_trans(transcript, key, comp,time)\n",
    "\n",
    "                            if sentence ==\"\":\n",
    "                                print(comp)\n",
    "                                continue\n",
    "\n",
    "                            if pos not in sent_dict:\n",
    "                                text.append(sentence[3:])\n",
    "                            sent_dict[pos] = sentence\n",
    "        \n",
    "                \n",
    "\n",
    "        file_summary[file.split(\".\")[0]] = \" \".join(tmp_sum)  \n",
    "        file_summary_order.append(\" \".join(tmp_sum))\n",
    "        all_text.append(\" \".join(text))  \n",
    "\n",
    "        #len_summ.append(len(\" \".join(tmp_sum).split()))\n",
    "        #len_input.append(len(\" \".join(text).split()))\n",
    "\n",
    "    assert len(all_text)==len(file_summary_order)\n",
    "    \n",
    "    assert len(file_summary)==len(r)\n",
    "    \n",
    "    if dtype=='test':\n",
    "        with open('bart_data_long/test_sum_gold.json', 'w') as f:\n",
    "            json.dump(file_summary, f)     \n",
    "\n",
    "    for i in range(len(file_summary)):\n",
    "        file1 = open(\"bart_data_long/cluster_\"+str(split_num)+\"_\"+dtype+\".txt.src\",\"a\")\n",
    "        file1.write(all_text[i]+\"\\n\")\n",
    "        file2 = open(\"bart_data_long/cluster_\"+str(split_num)+\"_\"+dtype+\".txt.tgt.tagged\",\"a\")\n",
    "        file2.write(file_summary_order[i]+\"\\n\")\n",
    "\n",
    "                  \n",
    "generate_data()                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "f9c7ef19",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_summ = []\n",
    "len_input = []\n",
    "def generate_data(split_num=4,dtype='train'):\n",
    "    avg = []\n",
    "    data = []\n",
    "    all_sum = []\n",
    "    f_name = []\n",
    "    count = 0 \n",
    " \n",
    "    f = open ('data-resplit/split.json', \"r\")  \n",
    "\n",
    "    data = json.loads(f.read())[str(split_num)]\n",
    "    r = data[dtype]\n",
    "\n",
    "    file_summary = {}\n",
    "    file_summary_order = []\n",
    "    all_text = []\n",
    "    for file in r:\n",
    "\n",
    "        dir = '/Users/heatherzheng/Desktop/ami_public_manual_1.6.2/abstractive/'\n",
    "        tree = ET.parse(dir+file)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        \n",
    "        transcript, key, corpus,_ = generate_transcript(files[file.split(\".\")[0]],speaker[file.split(\".\")[0]])\n",
    "        \n",
    "\n",
    "        \n",
    "        flag = True\n",
    "        group = 0\n",
    "\n",
    "        drda = []\n",
    "        stats = []\n",
    "\n",
    "        tmp_sum = []\n",
    "        \n",
    "        text = []\n",
    "        sent_dict = defaultdict(str)\n",
    "        for content in root.findall('decisions'):\n",
    "            for sentence in content.findall('sentence'):\n",
    "\n",
    "                summary = sentence.text.replace(\"\\n\",\"\")\n",
    "                tmp_sum.append(summary)\n",
    "\n",
    "                decision_id = sentence.attrib['{http://nite.sourceforge.net/}id']\n",
    "        \n",
    "                \n",
    "\n",
    "        file_summary[file.split(\".\")[0]] = \" \".join(tmp_sum)  \n",
    "        file_summary_order.append(\" \".join(tmp_sum))\n",
    "        all_text.append(\" \".join(corpus))  \n",
    "\n",
    "        len_summ.append(len(\" \".join(tmp_sum).split()))\n",
    "        len_input.append(len(\" \".join(corpus).split()))\n",
    "\n",
    "    assert len(all_text)==len(file_summary_order)\n",
    "    \n",
    "    assert len(file_summary)==len(r)\n",
    "    \n",
    "    if dtype=='test':\n",
    "        with open('bart_data_all/test_sum_gold'+str(split_num)+'.json', 'w') as f:\n",
    "            json.dump(file_summary, f)     \n",
    "\n",
    "    for i in range(len(file_summary)):\n",
    "        file1 = open(\"bart_data_all/cluster_\"+str(split_num)+\"_\"+dtype+\".txt.src\",\"a\")\n",
    "        file1.write(all_text[i]+\"\\n\")\n",
    "        file2 = open(\"bart_data_all/cluster_\"+str(split_num)+\"_\"+dtype+\".txt.tgt.tagged\",\"a\")\n",
    "        file2.write(file_summary_order[i]+\"\\n\")\n",
    "\n",
    "                  \n",
    "generate_data()                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edcbba99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5518.985507246377"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len_input)/len(len_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e01635c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11115"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81e307a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed\n",
    "from collections import defaultdict\n",
    "len_summ = []\n",
    "len_input = []\n",
    "verb_count = defaultdict(int)\n",
    "def generate_data(split_num=1,dtype='train'):\n",
    "    avg = []\n",
    "    data = []\n",
    "    all_sum = []\n",
    "    f_name = []\n",
    "    count = 0 \n",
    " \n",
    "    f = open ('data-resplit/split.json', \"r\")  \n",
    "\n",
    "    data = json.loads(f.read())[str(split_num)]\n",
    "    r = data[dtype]\n",
    "\n",
    "    file_summary = {}\n",
    "    file_summary_order = []\n",
    "    tmp_sum = []\n",
    "    tmp_input = []\n",
    "    all_text = []\n",
    "    \n",
    "    \n",
    "    for file in r:\n",
    "\n",
    "        dir = '/Users/heatherzheng/Desktop/ami_public_manual_1.6.2/abstractive/'\n",
    "        tree = ET.parse(dir+file)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        \n",
    "        transcript, key, corpus,_ = generate_transcript(files[file.split(\".\")[0]],speaker[file.split(\".\")[0]])\n",
    "        \n",
    "\n",
    "                    \n",
    "\n",
    "        \n",
    "        flag = True\n",
    "        group = 0\n",
    "\n",
    "        drda = []\n",
    "        stats = []\n",
    "\n",
    "        \n",
    "        \n",
    "        text = []\n",
    "        sent_dict = defaultdict(str)\n",
    "        for content in root.findall('decisions'):\n",
    "            for sentence in content.findall('sentence'):\n",
    "\n",
    "                summary = sentence.text.replace(\"\\n\",\"\")\n",
    "                tmp_sum.append(summary)\n",
    "                decision_id = sentence.attrib['{http://nite.sourceforge.net/}id']\n",
    "                #print(decision_id)\n",
    "                ext_link = link_map[decision_id]\n",
    "                '''\n",
    "                if ext_link ==[]:\n",
    "                    flag = False\n",
    "                else:\n",
    "                    for i in ext_link:\n",
    "                        left,right,_ = get_drda(file, i)\n",
    "                        da,time = get_da(left,right)\n",
    "                        das = sent_detector.tokenize(da)\n",
    "                        for comp in das:\n",
    "                            if 'just kinetic then' in comp:\n",
    "                                comp = 'just kinetic then'\n",
    "                            if len(comp.split())<3:\n",
    "                                continue\n",
    "                            pos, sentence, savedkey = search_trans(transcript, key, comp,time)\n",
    "                            tmp_input.append(sentence)\n",
    "                            s = sentence.split(\" \")\n",
    "                            s_pos_tag = pos_tag(s)\n",
    "                            s = [lemmatizer.lemmatize(i,j[0].lower()) if j[0].lower() in ['a','n','v'] else lemmatizer.lemmatize(i) for i,j in s_pos_tag]\n",
    "                            for w,tag in zip(s,s_pos_tag):\n",
    "                                if tag[1][0].lower() in ['v']:\n",
    "                                    verb_count[w] +=1\n",
    "                            \n",
    "                '''\n",
    "    with open('train_'+str(split_num)+'_summary_input.json', 'w') as f:\n",
    "        json.dump(tmp_sum, f)\n",
    "for i in range(5):                  \n",
    "    generate_data(i)                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "b7de81bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file = open(\"/Users/heatherzheng/Downloads/word_0_new.json\")\n",
    "seed=json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e7bef3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/heatherzheng/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/heatherzheng/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/heatherzheng/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/heatherzheng/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.056082811722816615\n",
      "0.6876011488511489\n",
      "0.10016360883733014\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "import json\n",
    "import csv\n",
    "sentence_summary_map = {}\n",
    "max_len = []\n",
    "def ntm_data(split_num=0,dtype='test'):\n",
    "\n",
    "    \n",
    "    f = open ('data-resplit/split.json', \"r\")  \n",
    "\n",
    "    data = json.loads(f.read())[str(split_num)]\n",
    "    r = data[dtype]\n",
    "    pe,re=0,0\n",
    "    f1 = 0\n",
    "    num =0\n",
    "\n",
    "    for file in r:\n",
    "        num +=1\n",
    "        dir = '/Users/heatherzheng/Desktop/ami_public_manual_1.6.2/abstractive/'\n",
    "        tree = ET.parse(dir+file)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        transcript, key, _, _= generate_transcript(files[file.split('.')[0]],speaker[file.split('.')[0]])\n",
    "\n",
    "        sent_dict = defaultdict(str)\n",
    "        flag = True\n",
    "        group = 0\n",
    "        \n",
    "        all_drs = []\n",
    "        predict_true,real_true,correct =0,0,0\n",
    "        for content in root.findall('decisions'):\n",
    "            for sentence in content.findall('sentence'):\n",
    "                summary = sentence.text.replace(\"\\n\",\"\")\n",
    "                group += 1\n",
    "                #f_name.append(file)\n",
    "                decision_id = sentence.attrib['{http://nite.sourceforge.net/}id']\n",
    "                #print(decision_id)\n",
    "                ext_link = link_map[decision_id]\n",
    "\n",
    "                if ext_link ==[]:\n",
    "                    flag = False\n",
    "                else:\n",
    "                    for i in ext_link:\n",
    "                        left,right,_ = get_drda(file, i)\n",
    "                        da,time = get_da(left,right)\n",
    "                        das = sent_detector.tokenize(da)\n",
    "                        for comp in das:\n",
    "                            if 'just kinetic then' in comp:\n",
    "                                comp = 'just kinetic then'\n",
    "                            if len(comp.split())<3:\n",
    "                                continue\n",
    "                            pos, sentence, savedkey = search_trans(transcript, key, comp,time)\n",
    "                            \n",
    "\n",
    "                            if sentence ==\"\":\n",
    "                                print(comp)\n",
    "                                continue\n",
    "\n",
    "                            if len(sentence.split())<3:\n",
    "                                continue\n",
    "\n",
    "                            if pos not in sent_dict:\n",
    "                                sentence_summary_map[sentence] = summary\n",
    "                                all_drs.append(sentence[3:])\n",
    "                                #tmp.append(sentence)\n",
    "                            sent_dict[pos] = sentence\n",
    "\n",
    "        for i in key:\n",
    "            \n",
    "            sentence = transcript[i][2:].lower()\n",
    "            \n",
    "            sentence = \" \".join([i for i in sentence.split() if i not in utterance])\n",
    "\n",
    "            if len(sentence.split())<3:\n",
    "                continue\n",
    "                \n",
    "\n",
    "            predict= False\n",
    "            real = False\n",
    "            \n",
    "            if sentence in all_drs:\n",
    "                real = True       \n",
    "                \n",
    "            sentence = sentence.split()\n",
    "            sentence = [lemmatizer.lemmatize(i,j[0].lower()) if j[0].lower() in ['a','n','v'] else lemmatizer.lemmatize(i) for i,j in pos_tag(sentence)]\n",
    "\n",
    "            tc = 0\n",
    "            for word in sentence:\n",
    "                if word in seed:\n",
    "                    tc +=1\n",
    "                    \n",
    "            if tc>=1:\n",
    "                predict = True\n",
    "\n",
    "            if predict and real:\n",
    "                correct +=1\n",
    "            if predict:\n",
    "                predict_true +=1\n",
    "            if real:\n",
    "                real_true += 1\n",
    "\n",
    "        tmpp = correct*1.0/predict_true\n",
    "        pe += tmpp\n",
    "        \n",
    "        tmpr= 0\n",
    "        if real_true!=0:\n",
    "            tmpr =  correct*1.0/real_true\n",
    "        else:\n",
    "            tmpr =1\n",
    "        re += tmpr\n",
    "        if tmpr+tmpp!=0:\n",
    "            f1 += 2*tmpr*tmpp/(tmpr+tmpp)\n",
    "        else:\n",
    "            f1+=0\n",
    "    print(pe/num)\n",
    "    print(re/num)\n",
    "    print(f1/num)\n",
    "                 \n",
    "\n",
    "    #print(true_count)\n",
    "    #print(len(train_label))\n",
    "\n",
    "ntm_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "5640e5ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[3]\n",
      "[4]\n",
      "[5]\n",
      "[5]\n",
      "[5]\n",
      "[5]\n",
      "[5]\n",
      "[1]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n",
      "[2]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-165-bed9430c333e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m \u001b[0mgenerate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-165-bed9430c333e>\u001b[0m in \u001b[0;36mgenerate_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m                                         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                                             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                                         \u001b[0mcontext_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfind_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcorpus_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m                                         \u001b[0;31m#text.append((s_time,sentence,group,pos,context_s,summary))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                                         \u001b[0mtextdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms_time\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcontext_s\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-165-bed9430c333e>\u001b[0m in \u001b[0;36mfind_context\u001b[0;34m(sent, corpus_tfidf)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmax_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_tfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus_tfidf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mmax_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1184\u001b[0m         \u001b[0mY_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_normalized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m         \u001b[0mY_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m     K = safe_sparse_dot(X_normalized, Y_normalized.T,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(X, norm, axis, copy, return_norm)\u001b[0m\n\u001b[1;32m   1902\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'%d' is not a supported axis\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1904\u001b[0;31m     X = check_array(X, accept_sparse=sparse_format, copy=copy,\n\u001b[0m\u001b[1;32m   1905\u001b[0m                     estimator='the normalize function', dtype=FLOAT_DTYPES)\n\u001b[1;32m   1906\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m             _assert_all_finite(array,\n\u001b[0m\u001b[1;32m    721\u001b[0m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[1;32m    722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;31m# safely to reduce dtype induced overflows.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mis_float\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m'fc'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mis_float\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_safe_accumulator_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_float\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#cluster train merge\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "\n",
    "all_summaries = []\n",
    "def find_context(sent, corpus_tfidf):\n",
    "    max_val = 0\n",
    "    max_pos = None\n",
    "    for i in range(len(corpus_tfidf)):\n",
    "        val = cosine_similarity(sent,[corpus_tfidf[i]])[0][0]\n",
    "        if val > max_val:\n",
    "            max_val = val\n",
    "            max_pos = i\n",
    "            \n",
    "    second_val = -0.1\n",
    "    second_pos = None\n",
    "    \n",
    "    for i in range(len(corpus_tfidf)):\n",
    "        val = cosine_similarity(sent,[corpus_tfidf[i]])[0][0]\n",
    "        if i==max_pos:\n",
    "            continue\n",
    "        if val > second_val:\n",
    "            second_val = val\n",
    "            second_pos = i\n",
    "        \n",
    "    if second_pos ==None:\n",
    "        print(second_val)\n",
    "        print(len(corpus_tfidf))\n",
    "    return second_pos\n",
    "\n",
    "def generate_data():\n",
    "    avg = []\n",
    "    data = []\n",
    "    all_sum = []\n",
    "    f_name = []\n",
    "    total_repeat = 0\n",
    "    \n",
    "    for cross_split in [1]:\n",
    "        tfidf_input = []\n",
    "        tfidf = None\n",
    "        for mode in [\"train\",'val','test']:\n",
    "            try:\n",
    "                os.mkdir('merge_group_'+str(cross_split))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "            try:\n",
    "                os.mkdir('merge_group_'+str(cross_split)+\"/\"+mode)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "                \n",
    "            pos_example, neg_example = 0,0\n",
    "\n",
    "            \n",
    "            f = open ('data-resplit/split.json', \"r\")  \n",
    "\n",
    "            data = json.loads(f.read())[str(cross_split)]\n",
    "            r = data[mode]\n",
    "\n",
    "            if tfidf_input == []:\n",
    "                for file in data['train']:\n",
    "                    _,_, corpus,_ = generate_transcript(files[file.split('.')[0]],speaker[file.split('.')[0]])\n",
    "                    tfidf_input.extend(corpus)\n",
    "                with open('merge_group_'+str(cross_split)+\"/\"+\"tfidf.json\", 'w') as f:\n",
    "                    json.dump(tfidf_input, f)               \n",
    "                vectorizer = TfidfVectorizer()\n",
    "                tfidf = vectorizer.fit(tfidf_input)\n",
    "                    \n",
    "            for file in r:\n",
    "                dir = '/Users/heatherzheng/Desktop/ami_public_manual_1.6.2/abstractive/'\n",
    "                tree = ET.parse(dir+file)\n",
    "                root = tree.getroot()\n",
    "\n",
    "                \n",
    "                transcript, key, corpus,_ = generate_transcript(files[file.split('.')[0]],speaker[file.split('.')[0]])\n",
    "                \n",
    "                corpus_tfidf = tfidf.transform(corpus)\n",
    "                textdict = {}\n",
    "            \n",
    "                flag = True\n",
    "                group = 0\n",
    "                \n",
    "                drda = []\n",
    "                stats = []\n",
    "                \n",
    "                sent_dict = defaultdict(str)\n",
    "                group_info = defaultdict(set)\n",
    "                group_to_sum = {}\n",
    "                for content in root.findall('decisions'):\n",
    "                    for sentence in content.findall('sentence'):\n",
    "                        sent_dict = defaultdict(str)\n",
    "                        \n",
    "                        summary = sentence.text.replace(\"\\n\",\"\")\n",
    "                        \n",
    "                        all_summaries.append(summary)\n",
    "                        num_sent = 0\n",
    "                        group += 1\n",
    "                        group_to_sum[group] = summary\n",
    "                        f_name.append(file)\n",
    "                        decision_id = sentence.attrib['{http://nite.sourceforge.net/}id']\n",
    "                        ext_link = link_map[decision_id]\n",
    "                        if ext_link ==[]:\n",
    "                            flag = False\n",
    "                        else:\n",
    "                            for i in ext_link:\n",
    "                                left,right,drda_type = get_drda(file, i)\n",
    "                                da,time = get_da(left,right)\n",
    "                                das = sent_detector.tokenize(da)\n",
    "                                for comp in das:\n",
    "\n",
    "                                    if 'just kinetic then' in comp:\n",
    "                                        comp = 'just kinetic then'\n",
    "                                    if len(comp.split())<3:\n",
    "                                        continue\n",
    "                                    if search_trans(transcript, key, comp,time)==None:\n",
    "                                        #print(comp)\n",
    "                                        #print(transcript)\n",
    "                                        comp = comp.replace(\". \",\"\")\n",
    "\n",
    "                                    pos, sentence, s_time = search_trans(transcript, key, comp,time)\n",
    "                                    #print(s_time,time)\n",
    "\n",
    "                                    if sentence ==\"\":\n",
    "                                        continue\n",
    "                                    \n",
    "                                    if pos not in sent_dict:\n",
    "                                        \n",
    "                                        if len(sentence.split())<3:\n",
    "                                            continue\n",
    "                                        context_s = corpus[find_context(tfidf.transform([sentence]).toarray(),corpus_tfidf.toarray())]\n",
    "                                        #text.append((s_time,sentence,group,pos,context_s,summary))\n",
    "                                        textdict[s_time] = [sentence,pos,context_s]\n",
    "                                        stats.append(group)\n",
    "                                        drda.append(sentence)\n",
    "                                        num_sent +=1\n",
    "                                        group_info[s_time].add(group)\n",
    "                                    sent_dict[pos] = sentence\n",
    "  \n",
    "                        \n",
    "\n",
    "                        avg.append(num_sent)\n",
    "\n",
    "   \n",
    "                merge_group = []\n",
    "                for i in range(1,group+1):\n",
    "                    merge_group.append([i])\n",
    "                    \n",
    "                    \n",
    "                prev_group_info = group_info.copy()\n",
    "                for i in prev_group_info:\n",
    "                    if len(prev_group_info[i])>1:\n",
    "                        \n",
    "                        idx_pos = []\n",
    "                        mlst = []\n",
    "                        for j in prev_group_info[i]:\n",
    "                            for gidx, g in enumerate(merge_group):\n",
    "                                if j in g:\n",
    "                                    idx_pos.append(gidx)\n",
    "                                    mlst.extend(g)\n",
    "                        new_merge_group = []\n",
    "                        for idx,ele in enumerate(merge_group):\n",
    "                            if idx not in idx_pos:\n",
    "                                new_merge_group.append(ele)\n",
    "                        merge_group = new_merge_group\n",
    "\n",
    "                        merge_group.append(list(set(mlst)))\n",
    "\n",
    "                text = []\n",
    "                for k in textdict:\n",
    "                    group_of_this_sent = list(group_info[k])\n",
    "                    print(group_of_this_sent)\n",
    "                    summary = \" \".join([group_to_sum[i] for i in group_of_this_sent])\n",
    "                    gnum =None\n",
    "                    for idx,i in enumerate(merge_group):\n",
    "                        if group_of_this_sent[0] in i:\n",
    "                            gnum=idx\n",
    "                    text.append([k,textdict[k][0],gnum,textdict[k][1],textdict[k][2],summary])\n",
    "                    \n",
    "\n",
    "                \n",
    "                text.sort(key = lambda x: x[0]) \n",
    "                #pos/neg calculation\n",
    "                c = list(dict(Counter(stats)).values())\n",
    "                for i in range(len(c)):\n",
    "                    if c[i]>1:\n",
    "                        pos_example += c[i]*(c[i]-1)/2.0\n",
    "                    for j in range(i+1,len(c)):\n",
    "                        neg_example += c[i]*c[j]\n",
    "                                       \n",
    "                result = []\n",
    "         \n",
    "                for i in range(len(text)):\n",
    "                    #sentence_pos,time, sentence, group, pos over passage, context sentence, drdas,summaries\n",
    "                    result.append((i,text[i][0],text[i][1],text[i][2],text[i][3], text[i][4],\" \".join(drda),text[i][5]))\n",
    "                for i in text:\n",
    "                    with open('merge_group_'+str(cross_split)+\"/\"+mode+\"/\"+file.split(\".\")[0]+\".json\", 'w') as f:\n",
    "                        json.dump(result, f)\n",
    "        \n",
    "    print(total_repeat)\n",
    "    print(sum(avg)/len(avg))\n",
    "generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2648bb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#neural_topic \n",
    "#for conflict change pair training algo:\n",
    "#cluster train\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "\n",
    "all_summaries = []\n",
    "def find_context(sent, corpus_tfidf):\n",
    "    max_val = 0\n",
    "    max_pos = None\n",
    "    for i in range(len(corpus_tfidf)):\n",
    "        val = cosine_similarity(sent,[corpus_tfidf[i]])[0][0]\n",
    "        if val > max_val:\n",
    "            max_val = val\n",
    "            max_pos = i\n",
    "            \n",
    "    second_val = -0.1\n",
    "    second_pos = None\n",
    "    \n",
    "    for i in range(len(corpus_tfidf)):\n",
    "        val = cosine_similarity(sent,[corpus_tfidf[i]])[0][0]\n",
    "        if i==max_pos:\n",
    "            continue\n",
    "        if val > second_val:\n",
    "            second_val = val\n",
    "            second_pos = i\n",
    "        \n",
    "    if second_pos ==None:\n",
    "        print(second_val)\n",
    "        print(len(corpus_tfidf))\n",
    "    return second_pos\n",
    "\n",
    "def generate_data():\n",
    "    avg = []\n",
    "    data = []\n",
    "    all_sum = []\n",
    "    f_name = []\n",
    "    total_repeat = 0\n",
    "    \n",
    "    for cross_split in [2,3,4]:\n",
    "        tfidf_input = []\n",
    "        tfidf = None\n",
    "        for mode in [\"train\",'val','test']:\n",
    "            try:\n",
    "                os.mkdir('fix_pair_algo_'+str(cross_split))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "            try:\n",
    "                os.mkdir('fix_pair_algo_'+str(cross_split)+\"/\"+mode)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "                \n",
    "            pos_example, neg_example = 0,0\n",
    "\n",
    "            \n",
    "            f = open ('data-resplit/split.json', \"r\")  \n",
    "\n",
    "            data = json.loads(f.read())[str(cross_split)]\n",
    "            r = data[mode]\n",
    "\n",
    "            if tfidf_input == []:\n",
    "                for file in data['train']:\n",
    "                    _,_, corpus,_ = generate_transcript(files[file.split('.')[0]],speaker[file.split('.')[0]])\n",
    "                    tfidf_input.extend(corpus)\n",
    "                with open('fix_pair_algo_'+str(cross_split)+\"/\"+\"tfidf.json\", 'w') as f:\n",
    "                    json.dump(tfidf_input, f)               \n",
    "                vectorizer = TfidfVectorizer()\n",
    "                tfidf = vectorizer.fit(tfidf_input)\n",
    "                    \n",
    "            for file in r:\n",
    "                dir = '/Users/heatherzheng/Desktop/ami_public_manual_1.6.2/abstractive/'\n",
    "                tree = ET.parse(dir+file)\n",
    "                root = tree.getroot()\n",
    "\n",
    "                \n",
    "                transcript, key, corpus,_ = generate_transcript(files[file.split('.')[0]],speaker[file.split('.')[0]])\n",
    "                \n",
    "                corpus_tfidf = tfidf.transform(corpus)\n",
    "                textdict = {}\n",
    "            \n",
    "                flag = True\n",
    "                group = 0\n",
    "                \n",
    "                drda = []\n",
    "                stats = []\n",
    "                \n",
    "                sent_dict = defaultdict(str)\n",
    "                group_info = defaultdict(set)\n",
    "                group_to_sum = {}\n",
    "                for content in root.findall('decisions'):\n",
    "                    for sentence in content.findall('sentence'):\n",
    "                        sent_dict = defaultdict(str)\n",
    "                        \n",
    "                        summary = sentence.text.replace(\"\\n\",\"\")\n",
    "                        \n",
    "                        all_summaries.append(summary)\n",
    "                        num_sent = 0\n",
    "                        group += 1\n",
    "                        group_to_sum[group] = summary\n",
    "                        f_name.append(file)\n",
    "                        decision_id = sentence.attrib['{http://nite.sourceforge.net/}id']\n",
    "                        ext_link = link_map[decision_id]\n",
    "                        if ext_link ==[]:\n",
    "                            flag = False\n",
    "                        else:\n",
    "                            for i in ext_link:\n",
    "                                left,right,drda_type = get_drda(file, i)\n",
    "                                da,time = get_da(left,right)\n",
    "                                das = sent_detector.tokenize(da)\n",
    "                                for comp in das:\n",
    "\n",
    "                                    if 'just kinetic then' in comp:\n",
    "                                        comp = 'just kinetic then'\n",
    "                                    if len(comp.split())<3:\n",
    "                                        continue\n",
    "                                    if search_trans(transcript, key, comp,time)==None:\n",
    "                                        #print(comp)\n",
    "                                        #print(transcript)\n",
    "                                        comp = comp.replace(\". \",\"\")\n",
    "\n",
    "                                    pos, sentence, s_time = search_trans(transcript, key, comp,time)\n",
    "                                    #print(s_time,time)\n",
    "\n",
    "                                    if sentence ==\"\":\n",
    "                                        continue\n",
    "                                    \n",
    "                                    if pos not in sent_dict:\n",
    "                                        \n",
    "                                        if len(sentence.split())<3:\n",
    "                                            continue\n",
    "                                        context_s = corpus[find_context(tfidf.transform([sentence]).toarray(),corpus_tfidf.toarray())]\n",
    "                                        #text.append((s_time,sentence,group,pos,context_s,summary))\n",
    "                                        textdict[s_time] = [sentence,pos,context_s]\n",
    "                                        stats.append(group)\n",
    "                                        drda.append(sentence)\n",
    "                                        num_sent +=1\n",
    "                                        group_info[s_time].add(group)\n",
    "                                    sent_dict[pos] = sentence\n",
    "  \n",
    "                        \n",
    "\n",
    "                        avg.append(num_sent)\n",
    "                text = []\n",
    "                for k in group_info:\n",
    "                    thissum = \" \".join([group_to_sum[i] for i in group_info[k]])\n",
    "                    text.append([k,textdict[k][0],list(group_info[k]),textdict[k][1],textdict[k][2],thissum])\n",
    "\n",
    "\n",
    "                \n",
    "                text.sort(key = lambda x: x[0]) \n",
    "                #pos/neg calculation\n",
    "                c = list(dict(Counter(stats)).values())\n",
    "                for i in range(len(c)):\n",
    "                    if c[i]>1:\n",
    "                        pos_example += c[i]*(c[i]-1)/2.0\n",
    "                    for j in range(i+1,len(c)):\n",
    "                        neg_example += c[i]*c[j]\n",
    "                                       \n",
    "                result = []\n",
    "         \n",
    "                for i in range(len(text)):\n",
    "                    #sentence_pos,time, sentence, group, pos over passage, context sentence, drdas,summaries\n",
    "                    result.append((i,text[i][0],text[i][1],text[i][2],text[i][3], text[i][4],\" \".join(drda),text[i][5]))\n",
    "                for i in text:\n",
    "                    with open('fix_pair_algo_'+str(cross_split)+\"/\"+mode+\"/\"+file.split(\".\")[0]+\".json\", 'w') as f:\n",
    "                        json.dump(result, f)\n",
    "        \n",
    "    print(total_repeat)\n",
    "    print(sum(avg)/len(avg))\n",
    "generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "816736dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'button': '11', 'channel': '5', 'group': '25', 'euro': '2', 'number': '19', 'scroll': '5', 'price': '7', 'function': '14', 'twenty': '7', 'point': '17', 'fifty': '10', 'changeable': '20', 'display': '6', 'base': '12', 'selling': '7', 'cost': '2', 'rubber': '8', 'profit': '16', 'wheel': '5', 'logo': '4', 'design': '13', 'yellow': '15', 'device': '22', 'prototype': '16', 'corporate': '24', 'production': '2', 'battery': '0', 'user': '3', 'television': '29', 'twelve': '10', 'kinetic': '8', 'lcd': '8', 'recognition': '23', 'screen': '18', 'light': '9', 'menu': '21', 'volume': '26', 'colorful': '20', 'organic': '15', 'projected': '17', 'rechargeable': '0', 'station': '12', 'gray': '9', 'expert': '6', 'voice': '23', 'cutout': '4', 'separately': '19', 'marketing': '6', 'company': '4', 'shape': '15', 'advanced': '14', 'center': '11', 'aim': '16', 'order': '19', 'cover': '20', 'image': '24', 'speech': '23', 'control': '28', 'locator': '14', 'touch': '18', 'original': '27', 'product': '1', 'small': '22', 'swapping': '21', 'symbol': '21', 'turbo': '21', 'color': '13', 'power': '26', 'previous': '26', 'final': '27', 'optional': '27', 'mute': '21', 'team': '29', 'feature': '21', 'target': '27', 'stationary': '1', 'multiple': '1', 'ergonomic': '1', 'conventional': '1', 'normal': '1', 'recharging': '1', 'trendy': '1', 'regular': '1', 'simple': '1', 'special': '1', 'international': '1', 'chip': '1', 'latex': '1', 'push': '1', 'soft': '1', 'sample': '1', 'speaker': '1', 'teletext': '1', 'longer': '1', 'component': '1', 'material': '1', 'case': '1', 'plastic': '1'}\n",
      "{'button': '34', 'group': '32', 'channel': '8', 'euro': '11', 'number': '14', 'battery': '28', 'scroll': '16', 'fifty': '11', 'cost': '19', 'function': '0', 'point': '10', 'twenty': '6', 'product': '31', 'changeable': '26', 'price': '6', 'base': '7', 'display': '5', 'profit': '23', 'wheel': '16', 'rubber': '17', 'volume': '8', 'logo': '3', 'lcd': '17', 'company': '3', 'image': '4', 'recognizable': '4', 'power': '8', 'corporate': '4', 'device': '24', 'selling': '6', 'prototype': '21', 'user': '30', 'twelve': '11', 'production': '19', 'kinetic': '17', 'recognition': '18', 'television': '29', 'order': '12', 'screen': '25', 'light': '27', 'menu': '33', 'final': '32', 'yellow': '15', 'colorful': '26', 'projected': '10', 'station': '7', 'gray': '27', 'rechargeable': '28', 'voice': '18', 'cutout': '3', 'organic': '15', 'design': '1', 'separately': '14', 'sporting': '31', 'advanced': '0', 'center': '9', 'aim': '23', 'expert': '5', 'marketing': '5', 'control': '2', 'speech': '18', 'cover': '22', 'locator': '0', 'touch': '25', 'small': '24', 'lithium': '28', 'original': '1', 'swapping': '33', 'symbol': '33', 'turbo': '33', 'previous': '13', 'mute': '33', 'optional': '1', 'target': '1', 'feature': '20', 'team': '29', 'stationary': '2', 'multiple': '2', 'charging': '2', 'ergonomic': '2', 'colour': '2', 'conventional': '2', 'recharging': '2', 'trendy': '2', 'regular': '2', 'international': '2', 'push': '2', 'simple': '2', 'special': '2', 'chip': '2', 'large': '2', 'normal': '2', 'possibility': '2', 'soft': '2', 'standard': '2', 'latex': '2', 'shape': '2', 'sample': '2', 'speaker': '2', 'fruit': '2', 'vegetable': '2', 'project': '2', 'manager': '2', 'component': '2', 'material': '2', 'teletext': '2', 'longer': '2', 'case': '2', 'plastic': '2'}\n"
     ]
    }
   ],
   "source": [
    "#cluster test\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "\n",
    "all_summaries = []\n",
    "def find_context(sent, corpus_tfidf):\n",
    "    max_val = 0\n",
    "    max_pos = None\n",
    "    for i in range(len(corpus_tfidf)):\n",
    "        val = cosine_similarity(sent,[corpus_tfidf[i]])[0][0]\n",
    "        if val > max_val:\n",
    "            max_val = val\n",
    "            max_pos = i\n",
    "            \n",
    "    second_val = -0.1\n",
    "    second_pos = None\n",
    "    \n",
    "    for i in range(len(corpus_tfidf)):\n",
    "        val = cosine_similarity(sent,[corpus_tfidf[i]])[0][0]\n",
    "        if i==max_pos:\n",
    "            continue\n",
    "        if val > second_val:\n",
    "            second_val = val\n",
    "            second_pos = i\n",
    "        \n",
    "    if second_pos ==None:\n",
    "        print(second_val)\n",
    "        print(len(corpus_tfidf))\n",
    "    return second_pos\n",
    "\n",
    "def generate_data():\n",
    "    avg = []\n",
    "    data = []\n",
    "    all_sum = []\n",
    "    f_name = []\n",
    "    \n",
    "\n",
    "    \n",
    "    for cross_split in [3,4]:\n",
    "\n",
    "        file = open(\"/Users/heatherzheng/Downloads/word_\"+str(cross_split)+\".json\")\n",
    "        seed=json.load(file)\n",
    "        print(seed)\n",
    "        tfidf_input = []\n",
    "        tfidf = None\n",
    "        for mode in ['test','train','val']:\n",
    "            try:\n",
    "                os.mkdir('pair_sentence_filter_'+str(cross_split))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "            try:\n",
    "                os.mkdir('pair_sentence_filter_'+str(cross_split)+\"/\"+mode)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "                \n",
    "            pos_example, neg_example = 0,0\n",
    "\n",
    "            \n",
    "            f = open ('data-resplit/split.json', \"r\")  \n",
    "\n",
    "            data = json.loads(f.read())[str(cross_split)]\n",
    "            r = data[mode]\n",
    "\n",
    "\n",
    "            if tfidf_input == []:\n",
    "                for file in data['train']:\n",
    "                    _,_, corpus,_ = generate_transcript(files[file.split('.')[0]],speaker[file.split('.')[0]])\n",
    "                    tfidf_input.extend(corpus)\n",
    "                with open('pair_sentence_filter_'+str(cross_split)+\"/\"+\"tfidf.json\", 'w') as f:\n",
    "                    json.dump(tfidf_input, f)               \n",
    "                vectorizer = TfidfVectorizer()\n",
    "                tfidf = vectorizer.fit(tfidf_input)\n",
    "                    \n",
    "            for file in r:\n",
    "                dir = '/Users/heatherzheng/Desktop/ami_public_manual_1.6.2/abstractive/'\n",
    "                tree = ET.parse(dir+file)\n",
    "                root = tree.getroot()\n",
    "\n",
    "                \n",
    "                transcript, key, corpus,_ = generate_transcript(files[file.split('.')[0]],speaker[file.split('.')[0]])\n",
    "                \n",
    "                corpus_tfidf = tfidf.transform(corpus)\n",
    "                text = []\n",
    "            \n",
    "                flag = True\n",
    "                group = 0\n",
    "                \n",
    "                drda = []\n",
    "                stats = []\n",
    "                inputs = []\n",
    "    \n",
    "                for content in root.findall('decisions'):\n",
    "                    for sentence in content.findall('sentence'):\n",
    "                        summary = sentence.text.replace(\"\\n\",\"\")\n",
    "                        decision_id = sentence.attrib['{http://nite.sourceforge.net/}id']\n",
    "                        ext_link = link_map[decision_id]\n",
    "                        tmp = []\n",
    "                        if ext_link ==[]:\n",
    "                            flag = False\n",
    "                        else:\n",
    "                            for i in ext_link:\n",
    "                                left,right,drda_type = get_drda(file, i)\n",
    "                                da,time = get_da(left,right)\n",
    "                                das = sent_detector.tokenize(da)\n",
    "                                for comp in das:\n",
    "                                    if len(comp.split())<3:\n",
    "                                        continue\n",
    "                                    if 'just kinetic then' in comp:\n",
    "                                        comp = 'just kinetic then'\n",
    "                                    if search_trans(transcript, key, comp,time)==None:\n",
    "                                        #print(comp)\n",
    "                                        #print(transcript)\n",
    "                                        comp = comp.replace(\". \",\"\")\n",
    "\n",
    "\n",
    "                                    pos, sentence, s_time = search_trans(transcript, key, comp,time)\n",
    "                                    tmp.append([s_time,summary])\n",
    "                             \n",
    "                        \n",
    "                     \n",
    "                        inputs.append(tmp)  \n",
    "                      \n",
    "                count = 0\n",
    "                sum_map = {}\n",
    "                for i in inputs:\n",
    "                    for j in i:\n",
    "                        sum_map[j[0]]=[count,j[1]]\n",
    "                    count +=1\n",
    "                    \n",
    "                \n",
    "                for index,k in enumerate(key):\n",
    "                    ori_sent = [i.lower() for i in transcript[k][2:].split() if i.lower() not in utterance]\n",
    "                    sent = \" \".join(ori_sent)\n",
    "                    \n",
    "                    sentence = [lemmatizer.lemmatize(i,j[0].lower()) if j[0].lower() in ['a','n','v'] else lemmatizer.lemmatize(i) for i,j in pos_tag(ori_sent)]\n",
    "\n",
    "                    select =False\n",
    "                    for word in sentence:\n",
    "                        if word in seed:\n",
    "                            select=True\n",
    "                    if select:\n",
    "                        context_s = corpus[find_context(tfidf.transform([sent]).toarray(),corpus_tfidf.toarray())]\n",
    "                        if k in sum_map:\n",
    "                            text.append((k,sent,sum_map[k][0],index,context_s,sum_map[k][1]))\n",
    "                        else:\n",
    "                            text.append((k,sent,count,index,context_s,\"\"))\n",
    "                            \n",
    "                text.sort(key = lambda x: x[0]) \n",
    "                \n",
    "                result = []\n",
    "                for i in range(len(text)):\n",
    "                    #sentence_pos,time, sentence, group, pos over passage, context sentence, drdas,summaries\n",
    "                    result.append((i,text[i][0],text[i][1],text[i][2],text[i][3], text[i][4],\" \".join(drda),text[i][5]))\n",
    "                for i in text:\n",
    "                    with open('pair_sentence_filter_'+str(cross_split)+\"/\"+mode+\"/\"+file.split(\".\")[0]+\".json\", 'w') as f:\n",
    "                        json.dump(result, f)\n",
    "\n",
    "\n",
    "generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3a2bd17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "import bcubed\n",
    "from math import log\n",
    "\n",
    "def variation_of_information(X, Y):\n",
    "  n = float(sum([len(x) for x in X]))\n",
    "  sigma = 0.0\n",
    "  for x in X:\n",
    "    p = len(x) / n\n",
    "    for y in Y:\n",
    "      q = len(y) / n\n",
    "      r = len(set(x) & set(y)) / n\n",
    "      if r > 0.0:\n",
    "        sigma += r * (log(r / p, 2) + log(r / q, 2))\n",
    "  return abs(sigma)\n",
    "\n",
    "def lst_to_pair(lst):\n",
    "    pair = []\n",
    "    for i in lst:\n",
    "        if len(i)==1:\n",
    "            continue\n",
    "\n",
    "        for j in range(len(i)):\n",
    "            for k in range(j+1, len(i)):\n",
    "                pair.append((min(i[j],i[k]),max(i[j],i[k])))\n",
    "\n",
    "    return pair\n",
    "\n",
    "def get_pair_score(predict,expect):\n",
    "    predict = lst_to_pair(predict)\n",
    "    expect = lst_to_pair(expect)\n",
    "\n",
    "\n",
    "    tp = 0\n",
    "    for i in predict:\n",
    "        if i in expect:\n",
    "            tp +=1\n",
    "    if expect:\n",
    "        r = tp/len(expect)\n",
    "    else:\n",
    "        r = 1\n",
    "\n",
    "    if predict:\n",
    "        p = tp/len(predict)\n",
    "    else:\n",
    "        p= 1\n",
    "\n",
    "\n",
    "    if p+r==0:\n",
    "        return p,r,0\n",
    "\n",
    "    return p,r,2*r*p/(r+p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6044673b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TS3007b.abssumm.xml\n",
      "1\n",
      "0.6637577048496166 0.8712312675070029 0.7260919304069178 0.4221968731751341 0.8108745421245421 0.4588520351232216 1.1016034983908976\n"
     ]
    }
   ],
   "source": [
    "#summary by filter cluster\n",
    "import bcubed\n",
    "#f = open ('fix_4ntest_cluster.json', \"r\")\n",
    "f = open ('/Users/heatherzheng/Desktop/Neural_Topic_Models/svm_4_cluster.json', \"r\")\n",
    "\n",
    "\n",
    "\n",
    "cluster = json.loads(f.read())\n",
    "\n",
    "r = set([i.split(\".\")[0]+\".abssumm.xml\" for i in cluster[1]])\n",
    "\n",
    "bprecision,brecall,bfscore=0,0,0\n",
    "voi = 0\n",
    "err = 0\n",
    "pairp,pairr,pairf = 0,0,0\n",
    "for file in r:\n",
    "    sents = []\n",
    "    for i in range(len(cluster[0])):\n",
    "\n",
    "        if file.split(\".\")[0] in cluster[1][i]:\n",
    "            sents.append(cluster[0][i])\n",
    "            \n",
    "\n",
    "    \n",
    "    dir = '/Users/heatherzheng/Desktop/ami_public_manual_1.6.2/abstractive/'\n",
    "    tree = ET.parse(dir+file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "\n",
    "    transcript, key, corpus,_ = generate_transcript(files[file.split('.')[0]],speaker[file.split('.')[0]])\n",
    "\n",
    "    text = []\n",
    "\n",
    "    flag = True\n",
    "    group = 0\n",
    "\n",
    "    drda = []\n",
    "    stats = []\n",
    "    inputs = defaultdict(set)\n",
    "\n",
    "    for content in root.findall('decisions'):\n",
    "        for sentence in content.findall('sentence'):\n",
    "            summary = sentence.text.replace(\"\\n\",\"\")\n",
    "            decision_id = sentence.attrib['{http://nite.sourceforge.net/}id']\n",
    "            ext_link = link_map[decision_id]\n",
    "            tmp = []\n",
    "            if ext_link ==[]:\n",
    "                flag = False\n",
    "            else:\n",
    "                for i in ext_link:\n",
    "                    left,right,drda_type = get_drda(file, i)\n",
    "                    da,time = get_da(left,right)\n",
    "                    das = sent_detector.tokenize(da)\n",
    "                    for comp in das:\n",
    "                        if len(comp.split())<3:\n",
    "                            continue\n",
    "                        if 'just kinetic then' in comp:\n",
    "                            comp = 'just kinetic then'\n",
    "                        if search_trans(transcript, key, comp,time)==None:\n",
    "                            #print(comp)\n",
    "                            #print(transcript)\n",
    "                            comp = comp.replace(\". \",\"\")\n",
    "\n",
    "\n",
    "                        pos, sentence, s_time = search_trans(transcript, key, comp,time)\n",
    "                        inputs[sentence[3:]].add(group)\n",
    "\n",
    "\n",
    "            group +=1\n",
    "    gold_group_num = group\n",
    "    map_sent_to_idx = {}\n",
    "    predict = defaultdict(set)\n",
    "    count = 0\n",
    "    for sent in enumerate(sents):\n",
    "        usefulcluster=False\n",
    "        for key in inputs:\n",
    "            if key in sent[1]:\n",
    "                if key not in map_sent_to_idx:\n",
    "                    map_sent_to_idx[key] = len(map_sent_to_idx)+1\n",
    "                predict[map_sent_to_idx[key]].add(count)\n",
    "                usefulcluster = True\n",
    "                \n",
    "\n",
    "        if usefulcluster:\n",
    "            count +=1\n",
    "        \n",
    "    gold = defaultdict(set)\n",
    "    for i in inputs:\n",
    "        if i in map_sent_to_idx:\n",
    "            gold[map_sent_to_idx[i]] = inputs[i]\n",
    "    gold = dict(gold)\n",
    "    predict = dict(predict)\n",
    "    \n",
    "    if gold=={}:\n",
    "        err += 1\n",
    "        print(file)\n",
    "        continue\n",
    "\n",
    "    tmp_bp = bcubed.precision(predict, gold)\n",
    "\n",
    "    bprecision += tmp_bp\n",
    "\n",
    "    tmp_br = bcubed.recall(predict, gold)\n",
    "    brecall += tmp_br\n",
    "    \n",
    "    bfscore += bcubed.fscore(tmp_bp,tmp_br)\n",
    "\n",
    "    gold_lst = [[] for _ in range(gold_group_num)]\n",
    "\n",
    "    for key in gold:\n",
    "        for group in gold[key]:\n",
    "            if key not in gold_lst[group]:\n",
    "                gold_lst[group].append(key)\n",
    "    predict_lst = [[] for _ in range(count)]\n",
    "    for key in predict:\n",
    "        for group in predict[key]:\n",
    "            if key not in predict_lst[group]:\n",
    "                predict_lst[group].append(key)\n",
    "            \n",
    "    gold_lst = [i for i in gold_lst if i!=[]]\n",
    "    predict_lst = [i for i in predict_lst if i!=[]]\n",
    "\n",
    "    pt,rt, ft= get_pair_score(predict_lst, gold_lst)\n",
    "\n",
    "    pairp += pt\n",
    "    pairr += rt\n",
    "    pairf += ft\n",
    "    voi += variation_of_information(predict_lst, gold_lst)\n",
    "print(err)\n",
    "print(bprecision/(len(r)-err),brecall/(len(r)-err),bfscore/(len(r)-err),\n",
    "      pairp/(len(r)-err),pairr/(len(r)-err),pairf/(len(r)-err), voi/(len(r)-err))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1debc5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pick balanced bad example\n",
    "#summary by filter cluster\n",
    "import bcubed\n",
    "\n",
    "sec='test'\n",
    "cross_split = '4'\n",
    "f = open ('fix_'+cross_split+sec+'_cluster.json', \"r\")\n",
    "\n",
    "\n",
    "cluster = json.loads(f.read())\n",
    "\n",
    "r = set([i.split(\".\")[0]+\".abssumm.xml\" for i in cluster[1]])\n",
    "\n",
    "pos_sample= []\n",
    "neg_sample = []\n",
    "for file in r:\n",
    "    sents = []\n",
    "    for i in range(len(cluster[0])):\n",
    "\n",
    "        if file.split(\".\")[0] in cluster[1][i]:\n",
    "            sents.append(cluster[0][i])\n",
    "            \n",
    "\n",
    "    \n",
    "    dir = '/Users/heatherzheng/Desktop/ami_public_manual_1.6.2/abstractive/'\n",
    "    tree = ET.parse(dir+file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "\n",
    "    transcript, key, corpus,_ = generate_transcript(files[file.split('.')[0]],speaker[file.split('.')[0]])\n",
    "\n",
    "    text = []\n",
    "\n",
    "    flag = True\n",
    "    group = 0\n",
    "\n",
    "    drda = []\n",
    "    stats = []\n",
    "    inputs = defaultdict(set)\n",
    "    \n",
    "    group_to_summary = {}\n",
    "    for content in root.findall('decisions'):\n",
    "        for sentence in content.findall('sentence'):\n",
    "            summary = sentence.text.replace(\"\\n\",\"\")\n",
    "            group_to_summary[group]=summary\n",
    "            decision_id = sentence.attrib['{http://nite.sourceforge.net/}id']\n",
    "            ext_link = link_map[decision_id]\n",
    "            tmp = []\n",
    "            if ext_link ==[]:\n",
    "                flag = False\n",
    "            else:\n",
    "                for i in ext_link:\n",
    "                    left,right,drda_type = get_drda(file, i)\n",
    "                    da,time = get_da(left,right)\n",
    "                    das = sent_detector.tokenize(da)\n",
    "                    for comp in das:\n",
    "                        if len(comp.split())<3:\n",
    "                            continue\n",
    "                        if 'just kinetic then' in comp:\n",
    "                            comp = 'just kinetic then'\n",
    "                        if search_trans(transcript, key, comp,time)==None:\n",
    "                            #print(comp)\n",
    "                            #print(transcript)\n",
    "                            comp = comp.replace(\". \",\"\")\n",
    "\n",
    "\n",
    "                        pos, sentence, s_time = search_trans(transcript, key, comp,time)\n",
    "                        inputs[sentence[3:]].add(group)\n",
    "\n",
    "\n",
    "            group +=1\n",
    "\n",
    "\n",
    "    for sent in enumerate(sents):\n",
    "        cur_group = set()\n",
    "\n",
    "        for key in inputs:\n",
    "            if key in sent[1]:\n",
    "                cur_group.update(inputs[key])\n",
    "\n",
    "    \n",
    "        if len(cur_group)>0:\n",
    "            summ = []\n",
    "            for g in cur_group:\n",
    "                summ.append(group_to_summary[g])\n",
    "            pos_sample.append([sent[1],\" \".join(summ)])\n",
    "        else:\n",
    "            neg_sample.append([sent[1],\"empty\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "801bc0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "select_neg = random.sample(range(len(neg_sample)), 2*len(pos_sample))\n",
    "\n",
    "result = []\n",
    "for i in range(len(pos_sample)):\n",
    "    result.append(pos_sample[i])\n",
    "    \n",
    "random.shuffle(result)\n",
    "\n",
    "for i in range(len(result)):\n",
    "    file1 = open(\"bart_from_step2_sum/cluster_\"+cross_split+\"_\"+sec+\".txt.src\",\"a\")\n",
    "    file1.write(result[i][0]+\"\\n\")\n",
    "    file2 = open(\"bart_from_step2_sum/cluster_\"+cross_split+\"_\"+sec+\".txt.tgt.tagged\",\"a\")\n",
    "    file2.write(result[i][1]+\"\\n\")    \n",
    "    \n",
    "for i in select_neg:\n",
    "    result.append(neg_sample[i])\n",
    "random.shuffle(result)\n",
    "for i in range(len(result)):\n",
    "    file1 = open(\"bart_from_step2/cluster_\"+cross_split+\"_\"+sec+\".txt.src\",\"a\")\n",
    "    file1.write(result[i][0]+\"\\n\")\n",
    "    file2 = open(\"bart_from_step2/cluster_\"+cross_split+\"_\"+sec+\".txt.tgt.tagged\",\"a\")\n",
    "    file2.write(result[i][1]+\"\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "277b871c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-162-1f6c7ef47c4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stopwords'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'punkt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'averaged_perceptron_tagger'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wordnet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/site-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[0m\n\u001b[1;32m    775\u001b[0m                 )\n\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincr_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_or_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m                 \u001b[0;31m# Error messages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mErrorMessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/site-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mincr_download\u001b[0;34m(self, info_or_id, download_dir, force)\u001b[0m\n\u001b[1;32m    627\u001b[0m         \u001b[0;31m# Look up the requested collection or package.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m             \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_or_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_or_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Error loading {info_or_id}: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/site-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m_info_or_id\u001b[0;34m(self, info_or_id)\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_info_or_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo_or_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_or_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_or_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0minfo_or_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/site-packages/nltk/downloader.py\u001b[0m in \u001b[0;36minfo\u001b[0;34m(self, id)\u001b[0m\n\u001b[1;32m   1007\u001b[0m         \"\"\"Return the ``Package`` or ``Collection`` record for the\n\u001b[1;32m   1008\u001b[0m         given item.\"\"\"\n\u001b[0;32m-> 1009\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1010\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_packages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_packages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/site-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m_update_index\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    950\u001b[0m         \u001b[0;31m# Download the index file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m         self._index = nltk.internals.ElementWrapper(\n\u001b[0;32m--> 952\u001b[0;31m             \u001b[0mElementTree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetroot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m         )\n\u001b[1;32m    954\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'urllib.Request'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0m\u001b[1;32m    543\u001b[0m                                   '_open', req)\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1396\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1397\u001b[0;31m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[0m\u001b[1;32m   1398\u001b[0m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[1;32m   1399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1354\u001b[0;31m                 h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[0m\u001b[1;32m   1355\u001b[0m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[1;32m   1356\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1254\u001b[0m                 encode_chunked=False):\n\u001b[1;32m   1255\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1256\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1300\u001b[0m             \u001b[0;31m# default charset of iso-8859-1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m     def request(self, method, url, body=None, headers={}, *,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb\"\\r\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessage_body\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    952\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mNotConnected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0;34m\"Connect to a host on a given (SSL) port.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tunnel_host\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    920\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[0;34m\"\"\"Connect to the host and port specified in __init__.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m         self.sock = self._create_connection(\n\u001b[0m\u001b[1;32m    923\u001b[0m             (self.host,self.port), self.timeout, self.source_address)\n\u001b[1;32m    924\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetsockopt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIPPROTO_TCP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTCP_NODELAY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py38/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    794\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msource_address\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 796\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    797\u001b[0m             \u001b[0;31m# Break explicitly a reference cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m             \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def delete_rep(inputs):\n",
    "    lst = []\n",
    "    #topic = open(\"/home/sz488/Neural_Topic_Models/seed_1_filter_noverb_35.json\")\n",
    "    topic = open(\"/Users/heatherzheng/Downloads/seed_1_filter_noverb_35.json\")\n",
    "\n",
    "    seed_cluser_map = json.load(topic)\n",
    "    for s in inputs:\n",
    "        tmp = []\n",
    "        s = word_tokenize(s)\n",
    "        s_pos_tag = pos_tag(s)\n",
    "        s = [lemmatizer.lemmatize(i,j[0].lower()) if j[0].lower() in ['a','n','v'] else lemmatizer.lemmatize(i) for i,j in s_pos_tag]\n",
    "  \n",
    "        for w in s:\n",
    "            if w in seed_cluser_map:\n",
    "                tmp.append(w.lower())\n",
    "  \n",
    "        lst.append(tmp)\n",
    "    print(lst)\n",
    "\n",
    "                \n",
    "    all_word = set()\n",
    "    sents = []\n",
    "    remove = set()\n",
    "    for i in range(len(lst)):\n",
    "        max_rep = 0\n",
    "        max_rep_sent = None\n",
    "        for j in range(len(lst)):\n",
    "            if i!=j:\n",
    "                count = 0\n",
    "                for word in lst[i]:\n",
    "                    if word in lst[j]:\n",
    "                        count +=1\n",
    "                if count>max_rep:\n",
    "                    max_rep = count\n",
    "                    max_rep_sent = j\n",
    "        if max_rep>len(lst[i])*0.9:\n",
    "            if len(inputs[i].split())<len(inputs[max_rep_sent].split()) or (len(inputs[i].split())==len(inputs[max_rep_sent].split()) and i<max_rep_sent):\n",
    "                remove.add(i)\n",
    "    print(remove)\n",
    "    ans = []\n",
    "    for idx,sent in enumerate(inputs):\n",
    "        if idx not in remove:\n",
    "            ans.append(sent)\n",
    "            \n",
    "                \n",
    "    \n",
    "            \n",
    "    return \" \".join(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "54a731b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = [\" The remote will feature a locator function that responds to\",\n",
    "\" The remote will feature a locator function that responds to\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "59eb4818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The remote will feature a locator function that responds to\n",
      " The remote will feature a locator function that responds to\n",
      "[['feature', 'locator', 'function'], ['feature', 'locator', 'function']]\n",
      "{0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The remote will feature a locator function that responds to'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for s in i:\n",
    "    print(s)\n",
    "delete_rep(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3f8d7ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge bart\n",
    "\n",
    "#cluster train\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "\n",
    "all_summaries = []\n",
    "def find_context(sent, corpus_tfidf):\n",
    "    max_val = 0\n",
    "    max_pos = None\n",
    "    for i in range(len(corpus_tfidf)):\n",
    "        val = cosine_similarity(sent,[corpus_tfidf[i]])[0][0]\n",
    "        if val > max_val:\n",
    "            max_val = val\n",
    "            max_pos = i\n",
    "            \n",
    "    second_val = -0.1\n",
    "    second_pos = None\n",
    "    \n",
    "    for i in range(len(corpus_tfidf)):\n",
    "        val = cosine_similarity(sent,[corpus_tfidf[i]])[0][0]\n",
    "        if i==max_pos:\n",
    "            continue\n",
    "        if val > second_val:\n",
    "            second_val = val\n",
    "            second_pos = i\n",
    "        \n",
    "    if second_pos ==None:\n",
    "        print(second_val)\n",
    "        print(len(corpus_tfidf))\n",
    "    return second_pos\n",
    "\n",
    "def generate_data():\n",
    "    avg = []\n",
    "    data = []\n",
    "    all_sum = []\n",
    "    f_name = []\n",
    "    total_repeat = 0\n",
    "    \n",
    "    for cross_split in [1]:\n",
    "        tfidf_input = []\n",
    "        tfidf = None\n",
    "        for mode in [\"train\"]:\n",
    "      \n",
    "            f = open ('data-resplit/split.json', \"r\")  \n",
    "\n",
    "            data = json.loads(f.read())[str(cross_split)]\n",
    "            r = data[mode]\n",
    "            all_text = []\n",
    "            all_summary = []\n",
    "                    \n",
    "            for file in r:\n",
    "                dir = '/Users/heatherzheng/Desktop/ami_public_manual_1.6.2/abstractive/'\n",
    "                tree = ET.parse(dir+file)\n",
    "                root = tree.getroot()\n",
    "\n",
    "                \n",
    "                transcript, key, corpus,_ = generate_transcript(files[file.split('.')[0]],speaker[file.split('.')[0]])\n",
    "\n",
    "                textdict = {}\n",
    "            \n",
    "                flag = True\n",
    "                group = 0\n",
    "                \n",
    "                drda = []\n",
    "                stats = []\n",
    "                \n",
    "                sent_dict = defaultdict(str)\n",
    "                group_info = defaultdict(set)\n",
    "                group_to_sum = {}\n",
    "                id_to_sent = {}\n",
    "                group_to_ids = defaultdict(list)\n",
    "                for content in root.findall('decisions'):\n",
    "                    for sentence in content.findall('sentence'):\n",
    "                        sent_dict = defaultdict(str)\n",
    "                        \n",
    "                        summary = sentence.text.replace(\"\\n\",\"\")\n",
    "                        \n",
    "                        all_summaries.append(summary)\n",
    "                        num_sent = 0\n",
    "                        group += 1\n",
    "                        group_to_sum[group] = summary\n",
    "                        f_name.append(file)\n",
    "                        decision_id = sentence.attrib['{http://nite.sourceforge.net/}id']\n",
    "                        ext_link = link_map[decision_id]\n",
    "                        if ext_link ==[]:\n",
    "                            flag = False\n",
    "                        else:\n",
    "                            for i in ext_link:\n",
    "                                left,right,drda_type = get_drda(file, i)\n",
    "                                da,time = get_da(left,right)\n",
    "                                das = sent_detector.tokenize(da)\n",
    "                                for comp in das:\n",
    "\n",
    "                                    if 'just kinetic then' in comp:\n",
    "                                        comp = 'just kinetic then'\n",
    "                                    if len(comp.split())<3:\n",
    "                                        continue\n",
    "                                    if search_trans(transcript, key, comp,time)==None:\n",
    "                                        #print(comp)\n",
    "                                        #print(transcript)\n",
    "                                        comp = comp.replace(\". \",\"\")\n",
    "\n",
    "                                    pos, sentence, s_time = search_trans(transcript, key, comp,time)\n",
    "                                    #print(s_time,time)\n",
    "\n",
    "                                    if sentence ==\"\":\n",
    "                                        continue\n",
    "                                    \n",
    "                                    if pos not in sent_dict:\n",
    "                                        \n",
    "                                        if len(sentence.split())<3:\n",
    "                                            continue\n",
    "                                        \n",
    "                                        id_to_sent[s_time] = sentence\n",
    "                                        group_to_ids[group].append(s_time)\n",
    "                                        \n",
    "                                    sent_dict[pos] = sentence\n",
    "  \n",
    "                        \n",
    "\n",
    "                        avg.append(num_sent)\n",
    "\n",
    "   \n",
    "                merge_group = []\n",
    "                for i in range(1,group+1):\n",
    "                    merge_group.append([i])\n",
    "                    \n",
    "                    \n",
    "                prev_group_info = group_info.copy()\n",
    "                for i in prev_group_info:\n",
    "                    if len(prev_group_info[i])>1:\n",
    "                        \n",
    "                        idx_pos = []\n",
    "                        mlst = []\n",
    "                        for j in prev_group_info[i]:\n",
    "                            for gidx, g in enumerate(merge_group):\n",
    "                                if j in g:\n",
    "                                    idx_pos.append(gidx)\n",
    "                                    mlst.extend(g)\n",
    "                        new_merge_group = []\n",
    "                        for idx,ele in enumerate(merge_group):\n",
    "                            if idx not in idx_pos:\n",
    "                                new_merge_group.append(ele)\n",
    "                        merge_group = new_merge_group\n",
    "\n",
    "                        merge_group.append(list(set(mlst)))\n",
    "\n",
    "\n",
    "                \n",
    "                for g in merge_group:\n",
    "                    sent_group = set()\n",
    "                    this_sum = []\n",
    "                    ans = []\n",
    "                    for i in g:\n",
    "                        this_sum.append(group_to_sum[i])\n",
    "                        for j in group_to_ids[i]:\n",
    "                            sent_group.add(j)\n",
    "                            \n",
    "\n",
    "                    for j in sent_group:\n",
    "                        ans.append(id_to_sent[j][3:])\n",
    "                    all_text.append(\" \".join(ans))\n",
    "                    text_len.append(len(\" \".join(ans).split()))\n",
    "                    sum_len.append(len(\" \".join(this_sum).split()))\n",
    "                    all_summary.append(\" \".join(this_sum))\n",
    "            for i in range(len(all_summary)):\n",
    "                file1 = open(\"bart_merge/cluster_\"+str(cross_split)+\"_\"+mode+\".txt.src\",\"a\")\n",
    "                file1.write(all_text[i]+\"\\n\")\n",
    "                file2 = open(\"bart_merge/cluster_\"+str(cross_split)+\"_\"+mode+\".txt.tgt.tagged\",\"a\")\n",
    "                file2.write(all_summary[i]+\"\\n\")    \n",
    "\n",
    "generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "58f19ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_len = []\n",
    "sum_len = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9afd5df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(sum_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "a2a79dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305\n",
      "___\n",
      "C: , now , we had as listed options we had speech recognition potentially , flat screen interface , lcd interface we also want to limit the number of buttons so we'll pretty much take that one as read .\n",
      "{1}\n",
      "___\n",
      "92\n",
      "___\n",
      "C: then you know they're gonna be fairly brightly coloured anyway , and we can have sort of a a a trimming as well , of the glow in the dark material , just as gimmickyness .\n",
      "{2}\n",
      "___\n",
      "18\n",
      "___\n",
      "C: so we'll stick with sort of programmability for the buttons that we do have .\n",
      "{0}\n",
      "___\n",
      "19\n",
      "___\n",
      "C: excellent , so we'll go with speech recognition , yeah ?\n",
      "{0}\n",
      "___\n",
      "1\n",
      "___\n",
      "C: , speech recognition , limited buttons , organic design .\n",
      "{0, 1, 2}\n",
      "___\n",
      "0\n",
      "___\n",
      "D: if it's not too expensive s i think it's a good gimmick .\n",
      "{2}\n",
      "___\n",
      "1\n",
      "___\n",
      "B: glow in dark .\n",
      "{2}\n",
      "___\n",
      "98\n",
      "defaultdict(<class 'str'>, {0: 'The remote will feature speech recognition and programmable channel and volume preferences.', 1: 'The number of button functions will be limited to make the remote user-friendly.', 2: 'The remote will have an organic shape and feature some yellow coloring and glow-in-the-dark material.'})\n"
     ]
    }
   ],
   "source": [
    "#cluster test\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "\n",
    "all_summaries = []\n",
    "def find_context(sent, corpus_tfidf):\n",
    "    max_val = 0\n",
    "    max_pos = None\n",
    "    for i in range(len(corpus_tfidf)):\n",
    "        val = cosine_similarity(sent,[corpus_tfidf[i]])[0][0]\n",
    "        if val > max_val:\n",
    "            max_val = val\n",
    "            max_pos = i\n",
    "            \n",
    "    second_val = -0.1\n",
    "    second_pos = None\n",
    "    \n",
    "    for i in range(len(corpus_tfidf)):\n",
    "        val = cosine_similarity(sent,[corpus_tfidf[i]])[0][0]\n",
    "        if i==max_pos:\n",
    "            continue\n",
    "        if val > second_val:\n",
    "            second_val = val\n",
    "            second_pos = i\n",
    "        \n",
    "    if second_pos ==None:\n",
    "        print(second_val)\n",
    "        print(len(corpus_tfidf))\n",
    "    return second_pos\n",
    "\n",
    "def generate_data():\n",
    "    avg = []\n",
    "    data = []\n",
    "    all_sum = []\n",
    "    f_name = []\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    file = 'ES2005b.abssumm.xml'\n",
    "    dir = '/Users/heatherzheng/Desktop/ami_public_manual_1.6.2/abstractive/'\n",
    "    tree = ET.parse(dir+file)\n",
    "    root = tree.getroot()\n",
    "    transcript, key, corpus,_ = generate_transcript(files[file.split('.')[0]],speaker[file.split('.')[0]])\n",
    "\n",
    "    text = []\n",
    "\n",
    "    flag = True\n",
    "    group = 0\n",
    "\n",
    "    drda = []\n",
    "    stats = []\n",
    "    inputs = []\n",
    "    sent_to_group=defaultdict(set)\n",
    "    group_to_sum = defaultdict(str)\n",
    "    for content in root.findall('decisions'):\n",
    "        for sentence in content.findall('sentence'):\n",
    "            summary = sentence.text.replace(\"\\n\",\"\")\n",
    "            group_to_sum[group]=summary\n",
    "            decision_id = sentence.attrib['{http://nite.sourceforge.net/}id']\n",
    "            ext_link = link_map[decision_id]\n",
    "            tmp = []\n",
    "            if ext_link ==[]:\n",
    "                flag = False\n",
    "            else:\n",
    "                for i in ext_link:\n",
    "                    left,right,drda_type = get_drda(file, i)\n",
    "                    da,time = get_da(left,right)\n",
    "                    das = sent_detector.tokenize(da)\n",
    "                    for comp in das:\n",
    "                        if len(comp.split())<3:\n",
    "                            continue\n",
    "                        if 'just kinetic then' in comp:\n",
    "                            comp = 'just kinetic then'\n",
    "                        if search_trans(transcript, key, comp,time)==None:\n",
    "                            #print(comp)\n",
    "                            #print(transcript)\n",
    "                            comp = comp.replace(\". \",\"\")\n",
    "\n",
    "\n",
    "                        pos, sentence, s_time = search_trans(transcript, key, comp,time)\n",
    "                        sent_to_group[sentence[3:]].add(group)\n",
    "                        tmp.append([s_time,summary])\n",
    "            group += 1\n",
    "\n",
    "    \n",
    "    #print(sent_to_group)\n",
    "    text = []\n",
    "\n",
    "    for index,k in enumerate(key):\n",
    "        ori_sent = [i.lower() for i in transcript[k][2:].split() if i.lower() not in utterance]\n",
    "\n",
    "        sent = transcript[k][:2]+\" \"+\" \".join(ori_sent)\n",
    "        text.append([k,sent])\n",
    "\n",
    "\n",
    "    text.sort(key = lambda x: x[0]) \n",
    "    \n",
    "    skipline = 0\n",
    "\n",
    "    all_prev = []\n",
    "    for i in text:\n",
    "\n",
    "        if i[1][3:] in sent_to_group:\n",
    "            print(skipline)\n",
    "            print(\"___\")\n",
    "            print(i[1])\n",
    "\n",
    "            print(sent_to_group[i[1][3:]])\n",
    "            print(\"___\")\n",
    "            skipline = 0\n",
    "            \n",
    "            all_prev = []\n",
    "        else:\n",
    "            skipline +=1\n",
    "            all_prev.append(i[1])\n",
    "            #print(i[1])\n",
    "    print(skipline)\n",
    "    print(group_to_sum)\n",
    "\n",
    "\n",
    "generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618e3511",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": " create_dataset.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
